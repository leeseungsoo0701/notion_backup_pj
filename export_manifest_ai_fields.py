#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
export_manifest_ai_fields.py
- ì…ë ¥: manifest_dedup_sorted.csv (created_date,title,id,url)
- ì¶œë ¥: manifest_ai_enriched.csv (created_date,title,id,url,perf_flag,importance,summary_5lines)
- ê¸°ëŠ¥:
  * Notion í˜ì´ì§€ ë³¸ë¬¸ì„ Markdownìœ¼ë¡œ ìˆ˜ì§‘(ë¸”ë¡ í˜ì´ì§€ë„¤ì´ì…˜)
  * OpenAIë¡œ 'ì„±ê³¼ì— ì“¸ë§Œí•œì§€' ê¸°ì¤€ 5ì¤„ ìš”ì•½ + ì¤‘ìš”ë„(1~5) + perf_flag(yes/no)
  * --resume: ê¸°ì¡´ ì¶œë ¥ì´ ìˆìœ¼ë©´ ì´ì–´ì„œ(ì´ë¯¸ ì²˜ë¦¬í•œ idëŠ” ìŠ¤í‚µ)
  * --force / --force-retry-failed / --only-missing ë¡œ ì¬ì²˜ë¦¬ ì„ íƒ
  * ì§„í–‰ ë¡œê·¸(í–‰ ë‹¨ìœ„/êµ¬ê°„ ë‹¨ìœ„), ìë™ ì €ì¥, ì˜ˆì™¸ ìƒì„¸ ë¡œê·¸
  * latin-1 í—¤ë” ë³´í˜¸(í† í°ì— ë¹„ASCIIê°€ ì„ì¼ ë•Œ urllib3 UnicodeEncodeError ë°©ì§€)
"""

import os
import sys
import csv
import json
import time
import signal
import re
from typing import Dict, List, Set, Optional, Tuple
from datetime import datetime

import requests

# -------------------- í™˜ê²½/ì„¤ì • --------------------
BASE_URL = "https://api.notion.com/v1"
NOTION_VERSION = "2022-06-28"
API_TOKEN = os.environ.get("NOTION_TOKEN", "").strip() or "PUT_YOUR_INTEGRATION_TOKEN_HERE"

OUT_DIR = "./out"
FAIL_LOG = os.path.join(OUT_DIR, "failures_index.txt")

_session = requests.Session()
_session.trust_env = True

# OpenAI í˜¸ì¶œ ê°„ ìµœì†Œ ê°„ê²©(Throttle) ê´€ë¦¬
_last_ai_ts: float = 0.0

def _throttle(min_interval: float):
    global _last_ai_ts
    if min_interval <= 0:
        return
    now = time.time()
    wait = (_last_ai_ts + min_interval) - now
    if wait > 0:
        time.sleep(wait)
    _last_ai_ts = time.time()

# -------------------- ê³µìš© ìœ í‹¸ --------------------
def log(msg: str = ""):
    try:
        print(msg, flush=True)
    except Exception:
        print(str(msg).encode("ascii", "replace").decode("ascii"), flush=True)

def append_failure(path: str, msg: str, context: Optional[dict] = None):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "a", encoding="utf-8") as f:
        ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        line = {"ts": ts, "message": msg}
        if context:
            line["context"] = context
        f.write(json.dumps(line, ensure_ascii=False) + "\n")

def validate_token_ascii(token: str):
    try:
        _ = token.encode("latin-1")
    except Exception:
        log("â— HTTP í—¤ë” 'Authorization'ì— ë¹„-ASCII ë¬¸ìê°€ ì„ì˜€ìŠµë‹ˆë‹¤. NOTION_TOKEN ê°’ì„ ë‹¤ì‹œ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.")
        sys.exit(2)

def headers() -> Dict[str, str]:
    return {
        "Authorization": f"Bearer {API_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json",
    }

# -------------------- Notion API --------------------
def req(method: str, url: str, **kwargs):
    tries = 0
    while True:
        tries += 1
        try:
            r = _session.request(method, url, headers=headers(), timeout=60, **kwargs)
            if r.status_code in (429, 502, 503):
                time.sleep(min(1.0 * tries, 5))
                continue
            return r
        except Exception as e:
            if tries < 3:
                time.sleep(0.5 * tries)
                continue
            raise

def retrieve_page(page_id: str) -> Optional[dict]:
    r = req("GET", f"{BASE_URL}/pages/{page_id}")
    if r.status_code != 200:
        append_failure(FAIL_LOG, f"/pages/{page_id} ì‹¤íŒ¨ status={r.status_code}")
        return None
    return r.json()

def blocks_children(parent_id: str, start_cursor: Optional[str] = None, page_size: int = 100) -> dict:
    params = {"page_size": min(100, page_size)}
    if start_cursor:
        params["start_cursor"] = start_cursor
    r = req("GET", f"{BASE_URL}/blocks/{parent_id}/children", params=params)
    if r.status_code != 200:
        # 404ëŠ” ì‚­ì œ/ê¶Œí•œ/ì˜ëª»ëœID ë“±
        append_failure(FAIL_LOG, f"/blocks/{parent_id}/children ì‹¤íŒ¨ status={r.status_code}")
        return {"results": [], "has_more": False}
    return r.json()

# -------------------- Markdown ë¹Œë“œ --------------------
# ---- ë¡œì»¬ ìš”ì•½ê¸°(Heuristic Summarizer) ----
import math

_METRIC_PAT = re.compile(r"(\d{1,3}(?:[,\d]{0,3})+(?:\.\d+)?%?)|(%|\bp\d+\b|\bkr\d+\b)", re.I)
_KEYWORDS = [
    "ì„±ê³¼","ì§€í‘œ","ë°ì´í„°","ì „í™˜","ì „í™˜ìœ¨","ë§¤ì¶œ","êµ¬ë§¤","ê°€ì…","ì”ì¡´","ë¦¬í…ì…˜","KPI","OKR","KR",
    "ë‹¬ì„±","ê°œì„ ","ì¦ê°€","ê°ì†Œ","ìƒìŠ¹","í•˜ë½","ì¶œì‹œ","ë°°í¬","ëŸ°ì¹­","ë„ì…","ì‹¤í—˜","A/B","AB","ê°€ì„¤",
    "ê²°ì •","í•©ì˜","ìš°ì„ ìˆœìœ„","ë¦¬íŒ©í„°","ë¦¬íŒ©í† ë§","ë¹„ìš©","íš¨ìœ¨","ì†ë„","ì„±ëŠ¥","ë²„ê·¸","ì¥ì• ","í•´ê²°",
    "íš¨ê³¼","ì„íŒ©íŠ¸","ì˜í–¥","ëª©í‘œ","ê²°ê³¼","Outcome","Impact"
]

_NEGATIVE_HINTS = [
    "íšŒì˜ë¡","ì•ˆê±´","ëª©ì°¨","í…œí”Œë¦¿","ì‘ì„±ìš”ë ¹","ì˜ˆì‹œ","ìƒ˜í”Œ","TODO","To Do","to-do","ì°¸ê³ ","ë§í¬","ë§í¬ëª¨ìŒ",
    "ì œëª©ì—†ìŒ","ë¹ˆ í˜ì´ì§€","ë¹ˆí˜ì´ì§€","ë¯¸ì •","TBD","N/A"
]

# ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ (ì„±ê³¼ ê´€ì  ê°€ì¤‘ì¹˜) - ì‚¬ìš©ìê°€ ì§€ì •í•œ ë‹¨ì–´ ê¸°ë°˜
# ê°€ì¤‘ì¹˜ ê¸°ì¤€(ê¶Œì¥): 1=ë‚®ìŒ, 2=ë³´í†µ, 3=ë†’ìŒ
PRIORITY_KEYWORDS = {
    "ì„±ê³¼": 3,
    "ì „í™˜ìœ¨": 3,
    "roas": 3,       # ëŒ€ì†Œë¬¸ì ë¬´ì‹œ
    "abt": 2,        # ABT (A/B í…ŒìŠ¤íŠ¸ ê´€ë ¨)
    "a/b": 2,        # 'A/B' í‘œê¸°ë„ ë³´ê°•
    "ì‹¤í—˜": 2,
    "ë°°í¬ ê³µìœ ": 2,
    "íš¨ìœ¨í™”": 2,
    "ìë™í™”": 2,
    "prd": 2,
    "ë¬¸ì œ ì •ì˜": 2,
    "uxui": 1,
    "ìš”ì•½": 1,
}

def _clean_line(s: str) -> str:
    s = s.strip()
    # ë§ˆí¬ë‹¤ìš´ ë¨¸ë¦¬ê¸°í˜¸ ì œê±°
    s = re.sub(r"^#{1,6}\s*", "", s)   # headings
    s = re.sub(r"^[-*+]\s+", "", s)    # bullets
    s = re.sub(r"^\d+\.\s+", "", s)    # ordered list
    s = re.sub(r"^>\s*", "", s)        # quote
    s = s.replace("[x]", "").replace("[ ]", "")
    # ì½”ë“œ/êµ¬ë¶„ì„ /ë¹ˆ ì¤„ ì œê±°
    if s.startswith("```") or s == "---":
        return ""
    return s.strip()

def _score_line(s: str) -> int:
    if not s or len(s) < 4:
        return 0
    score = 0
    # í•µì‹¬ í‚¤ì›Œë“œ
    for kw in _KEYWORDS:
        if kw.lower() in s.lower():
            score += 2
            break
    # ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ (ì—¬ëŸ¬ ê°œ í¬í•¨ ì‹œ ëˆ„ì )
    low = s.lower()
    for kw, weight in PRIORITY_KEYWORDS.items():
        if kw in low:
            score += weight * 3  # ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œëŠ” ê°•í•˜ê²Œ ë°˜ì˜
    # ìˆ˜ì¹˜/í¼ì„¼íŠ¸/ì§€í‘œ
    if _METRIC_PAT.search(s):
        score += 2
    # ê°•í•œ ë™ì‚¬
    for v in ["ë‹¬ì„±","ê°œì„ ","ì¦ê°€","ê°ì†Œ","ê°ì†Œì‹œí‚´","ìƒìŠ¹","ì¶œì‹œ","ë°°í¬","ë„ì…","í•´ê²°","ì„±ê³¼","íš¨ìœ¨"]:
        if v in s:
            score += 1
            break
    # ê²°ì •/ì •ì±…/ìš°ì„ ìˆœìœ„
    for v in ["ê²°ì •","í•©ì˜","ì •ì±…","ê°€ì´ë“œ","ìš°ì„ ìˆœìœ„","ì¤‘ìš”", "í•µì‹¬"]:
        if v in s:
            score += 1
            break
    # ë¶€ì •ì /í˜•ì‹ì  ë¼ì¸ ê°ì 
    for ng in _NEGATIVE_HINTS:
        if ng.lower() in s.lower():
            score -= 1
            break
    # ë„ˆë¬´ ì§§ê±°ë‚˜ ë„ˆë¬´ ê¸¸ë©´ ì•½ê°„ ê°ì /ë³´ì •
    if len(s) > 220:
        score -= 1
    return max(score, 0)

def summarize_locally(markdown_text: str, title: str, created_date: str, url: str) -> dict:
    """
    ë§ˆí¬ë‹¤ìš´ì„ ê°€ë³ê²Œ ìŠ¤ìº”í•˜ì—¬ 'ì„±ê³¼ ê´€ì ' 5ì¤„ ìš”ì•½, perf_flag, importanceë¥¼ ìƒì„±.
    - í›„ë³´: í—¤ë”©/ë¦¬ìŠ¤íŠ¸/ìˆ«ìí¬í•¨/í‚¤ì›Œë“œ í¬í•¨ ë¼ì¸ ìœ„ì£¼
    - ì ìˆ˜: í‚¤ì›Œë“œ/ìˆ«ì/ë™ì‚¬/ê²°ì • í‚¤ì›Œë“œë¡œ ê°€ì¤‘
    """
    lines = []
    for raw in (markdown_text or "").splitlines():
        s = _clean_line(raw)
        if not s:
            continue
        # ì˜ë¯¸ ì—†ëŠ” ë¼ì¸ í•„í„°
        if s.lower().startswith(("http://","https://")):
            continue
        # callout ì´ëª¨ì§€ ì œê±° í”ì 
        s = s.replace("ğŸ’¡", "").strip()
        lines.append(s)

    # í›„ë³´ ì„ ë³„
    candidates = []
    for s in lines:
        base = 0
        if any(s.startswith(prefix) for prefix in ("#", "-", "1.", "2.", "3.", "OKR", "KR", "[", "â€¢", "Â·")):
            base += 1
        if any(kw.lower() in s.lower() for kw in _KEYWORDS) or _METRIC_PAT.search(s):
            base += 1
        score = _score_line(s) + base
        if score > 0:
            candidates.append((score, s))

    # ì¤‘ë³µ/ìœ ì‚¬ ë¬¸ì¥ ì œê±° ê°„ë‹¨ ì²˜ë¦¬
    seen = set()
    deduped = []
    def _priority_hit(t: str) -> int:
        tl = t.lower()
        return 1 if any(pk in tl for pk in PRIORITY_KEYWORDS.keys()) else 0

    for score, s in sorted(candidates, key=lambda x: (-x[0], -_priority_hit(x[1]), len(x[1]))):
        key = s.lower()
        if key in seen:
            continue
        seen.add(key)
        deduped.append((score, s))
        if len(deduped) >= 80:
            break

    # ìµœê³  ì ìˆ˜ ê¸°ë°˜ perf/importance ì‚°ì¶œ
    top_score = deduped[0][0] if deduped else 0
    perf_flag = "yes" if top_score >= 3 else ("yes" if any(k in (markdown_text or "").lower() for k in ["okr","kr","kpi","ì „í™˜","ë§¤ì¶œ"]) else "no")

    if top_score >= 7:
        importance = 5
    elif top_score >= 5:
        importance = 4
    elif top_score >= 3:
        importance = 3
    elif top_score >= 2:
        importance = 2
    else:
        importance = 1

    # ìƒìœ„ 5ì¤„ êµ¬ì„±
    top_lines = [s for _, s in deduped][:5]
    if len(top_lines) < 5:
        # ë³´ê°•: ì œëª©/ë‚ ì§œ/ë§í¬ë¡œ ì±„ì›€
        filler = [
            f"[ì œëª©] {title}".strip(),
            f"[ë‚ ì§œ] {created_date}".strip(),
            f"[ë§í¬] {url}".strip()
        ]
        for x in filler:
            if len(top_lines) >= 5: break
            if x and x not in top_lines:
                top_lines.append(x)
    # ê¸¸ì´ ì œí•œ
    top_lines = [ (l[:240] + "â€¦") if len(l) > 240 else l for l in top_lines ]
    # 5ê°œ ë§ì¶”ê¸°
    top_lines = (top_lines + [""]*5)[:5]

    return {"perf_flag": perf_flag, "importance": importance, "summary_5lines": top_lines}
def _rich_text_to_plain(rt_arr: list) -> str:
    buf = []
    for o in rt_arr or []:
        t = o.get("type")
        if t == "text":
            buf.append(o.get("text", {}).get("content", ""))
        elif t == "mention":
            m = o.get("mention", {}) or {}
            if m.get("type") == "user":
                name = (m.get("user", {}) or {}).get("name", "")
                buf.append(f"@{name}" if name else "@user")
            else:
                buf.append("@mention")
        elif t == "equation":
            buf.append(o.get("equation", {}).get("expression", ""))
    return "".join(buf)

def block_to_md(block: dict) -> str:
    t = block.get("type")
    b = block.get(t) or {}
    if t == "paragraph":
        return _rich_text_to_plain(b.get("rich_text"))
    if t in ("heading_1", "heading_2", "heading_3"):
        hashes = {"heading_1": "#", "heading_2": "##", "heading_3": "###"}[t]
        return f"{hashes} {_rich_text_to_plain(b.get('rich_text'))}"
    if t in ("bulleted_list_item", "numbered_list_item"):
        prefix = "-" if t == "bulleted_list_item" else "1."
        return f"{prefix} {_rich_text_to_plain(b.get('rich_text'))}"
    if t == "to_do":
        chk = "[x]" if b.get("checked") else "[ ]"
        return f"- {chk} {_rich_text_to_plain(b.get('rich_text'))}"
    if t == "quote":
        return f"> {_rich_text_to_plain(b.get('rich_text'))}"
    if t == "callout":
        return f"> ğŸ’¡ {_rich_text_to_plain(b.get('rich_text'))}"
    if t == "code":
        lang = b.get("language") or ""
        txt = _rich_text_to_plain(b.get("rich_text"))
        return f"```{lang}\n{txt}\n```"
    if t == "divider":
        return "---"
    if t == "bookmark":
        url = b.get("url", "")
        cap = _rich_text_to_plain(b.get("caption") or [])
        return f"[{cap or url}]({url})" if url else cap
    if t == "image":
        cap = _rich_text_to_plain(b.get("caption") or [])
        return f"![image]({cap})" if cap else "![image]"
    # child_page/child_database ë“±ì€ ìƒìœ„ì—ì„œ ë³„ë„ë¡œ ìˆœíšŒí•˜ë¯€ë¡œ ìŠ¤í‚µ
    return ""

def get_page_markdown(page_id: str, max_blocks: int = 2000) -> str:
    """ì²« max_blocks ë§Œí¼ 1ë‹¨ê³„ childrenì„ Markdownìœ¼ë¡œ ë‹¨ìˆœ ê²°í•©(ê¹Šì´ 1, ì†ë„ ìš°ì„ )"""
    lines: List[str] = []
    cursor = None
    scanned = 0
    while True and scanned < max_blocks:
        data = blocks_children(page_id, start_cursor=cursor, page_size=100)
        results = data.get("results", [])
        if not results:
            break
        for blk in results:
            if scanned >= max_blocks:
                break
            md = block_to_md(blk)
            if md:
                lines.append(md)
            scanned += 1
        if not data.get("has_more"):
            break
        cursor = data.get("next_cursor")
    return "\n".join(lines).strip()

# -------------------- OpenAI --------------------
# pip install openai
from openai import OpenAI
_openai_client = None

def get_openai():
    global _openai_client
    if _openai_client is None:
        _openai_client = OpenAI()  # í‚¤ëŠ” OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ì—ì„œ ìë™ ë¡œë“œ
    return _openai_client

def parse_ai_output(text: str) -> dict:
    """
    ëª¨ë¸ì´ ì•„ë˜ JSONì„ ë°˜í™˜í•œë‹¤ê³  ê°€ì •:
    {
      "perf_flag": "yes"|"no",
      "importance": 1..5,
      "summary_5lines": ["...","...","...","...","..."]
    }
    ì½”ë“œë¸”ë¡ ```json ... ``` í˜•íƒœë„ í—ˆìš©.
    """
    import json as _json, re as _re

    # ì½”ë“œë¸”ë¡ ì œê±°
    code_match = _re.search(r"```(?:json)?(.*?)```", text, _re.S | _re.I)
    if code_match:
        text = code_match.group(1).strip()

    # ì²« ë²ˆì§¸ JSON ì˜¤ë¸Œì íŠ¸ë§Œ ì¶”ì¶œ
    m = _re.search(r"\{.*\}", text, _re.S)
    raw = m.group(0) if m else text.strip()

    # JSON íŒŒì‹± (ì‹¤íŒ¨ ì‹œ ì•ˆì „ ê¸°ë³¸ê°’ ë°˜í™˜)
    try:
        data = _json.loads(raw)
    except Exception:
        return {"perf_flag": "no", "importance": 1, "summary_5lines": ["", "", "", "", ""]}

    perf = str(data.get("perf_flag", "no")).lower()
    perf = "yes" if perf in ("yes", "y", "true", "1") else "no"

    try:
        imp = int(data.get("importance", 1))
    except Exception:
        imp = 1
    imp = min(5, max(1, imp))

    lines = data.get("summary_5lines", [])
    if isinstance(lines, str):
        lines = [s.strip() for s in lines.split("\n") if s.strip()]
    if not isinstance(lines, list):
        lines = []
    lines = (lines + [""] * 5)[:5]

    return {"perf_flag": perf, "importance": imp, "summary_5lines": lines}

def call_openai_summary(
    markdown_text: str,
    model: str,
    timeout: int,
    debug: bool = False,
    max_retries: int = 8,
    initial_backoff: float = 1.0,
    max_backoff: float = 60.0,
    min_interval: float = 0.0,
) -> dict:
    """
    ì „ì²´ ë§ˆí¬ë‹¤ìš´ì„ ì½ê³  'ì„±ê³¼ì— ì“¸ë§Œí•œê°€' ê¸°ì¤€ìœ¼ë¡œ 5ì¤„ ìš”ì•½/ì¤‘ìš”ë„/ì„±ê³¼ì—¬ë¶€ë¥¼ JSONìœ¼ë¡œ ë°˜í™˜.
    429/5xx/ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ëŠ” ì§€ìˆ˜ ë°±ì˜¤í”„+ì§€í„°ë¡œ ì¬ì‹œë„. í˜¸ì¶œ ê°„ ìµœì†Œê°„ê²©(min_interval) ë³´ì¥.
    ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ê·¸ëŒ€ë¡œ ë˜ì§(ìƒìœ„ì—ì„œ ê¸°ë³¸ìš”ì•½ ì²˜ë¦¬).
    """
    client = get_openai()

    sys_prompt = (
        "You are a performance review assistant for a PM.\n"
        "Given a single Notion page (rendered as Markdown), read ALL content and decide:\n"
        "1) perf_flag: 'yes' if there is any content usable for performance review; otherwise 'no'.\n"
        "2) importance: integer 1~5 (5 is most impactful to performance review).\n"
        "3) summary_5lines: a 5-line summary capturing concrete achievements, metrics, decisions, and outcomes.\n"
        "Return ONLY valid JSON with keys: perf_flag, importance, summary_5lines (array of 5 strings)."
    )
    user_prompt = f"# Notion Markdown\n{markdown_text}\n"

    tries = 0
    last_err = None
    backoff = max(0.1, float(initial_backoff))

    while tries < max_retries:
        tries += 1
        try:
            _throttle(min_interval)
            resp = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=0.2,
                timeout=timeout,
            )
            text = resp.choices[0].message.content.strip()
            if debug:
                print("[OpenAI][raw]", text[:500], "..." if len(text) > 500 else "")
            return parse_ai_output(text)
        except KeyboardInterrupt:
            raise
        except Exception as e:
            # ì¿¼í„° ë¶€ì¡±ì€ ë¹ ë¥´ê²Œ ìƒìœ„ë¡œ
            es = str(e)
            if "insufficient_quota" in es or "You exceeded your current quota" in es:
                raise e
            last_err = e
            # ì§€ìˆ˜ ë°±ì˜¤í”„ + ì§€í„°
            wait = min(max_backoff, backoff) * (1.0 + (0.25 * (time.time() % 1)))
            time.sleep(wait)
            backoff *= 2

    raise last_err

# -------------------- Hugging Face Summarizer --------------------
_hf_summarizer = None

def get_hf_summarizer(model_name: str = "facebook/bart-large-cnn"):
    """
    Lazy-load a HF summarization pipeline. We import inside to avoid hard dependency
    when the user doesn't pass --use-hf.
    """
    global _hf_summarizer
    if _hf_summarizer is None:
        try:
            from transformers import pipeline
        except Exception as e:
            raise RuntimeError(
                "transformers íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. `pip install transformers torch` í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”."
            ) from e
        _hf_summarizer = pipeline("summarization", model=model_name)
    return _hf_summarizer

def _hf_chunk(text: str, chunk_chars: int = 3500, overlap: int = 200) -> List[str]:
    """
    Rough character-based chunking to avoid exceeding model max length.
    """
    if not text:
        return []
    chunks = []
    i = 0
    n = len(text)
    while i < n:
        j = min(n, i + chunk_chars)
        chunk = text[i:j]
        chunks.append(chunk)
        if j >= n:
            break
        i = j - overlap if j - overlap > i else j
    return chunks

def summarize_with_hf(markdown_text: str,
                      model_name: str = "facebook/bart-large-cnn",
                      max_length: int = 220,
                      min_length: int = 60) -> dict:
    """
    Summarize with Hugging Face pipeline.
    For long content: summarize chunks, then summarize the concatenated chunk-summaries.
    Returns the same schema as OpenAI path: {perf_flag, importance, summary_5lines}.
    """
    text = (markdown_text or "").strip()
    if not text:
        return {"perf_flag": "no", "importance": 1, "summary_5lines": ["", "", "", "", ""]}

    pipe = get_hf_summarizer(model_name=model_name)

    # 1) chunked summarization
    pieces = _hf_chunk(text, chunk_chars=3500, overlap=200)
    if not pieces:
        pieces = [text]

    inter_summaries = []
    for p in pieces:
        try:
            out = pipe(p, max_length=max_length, min_length=min_length, do_sample=False)
            inter_summaries.append(out[0]["summary_text"].strip())
        except Exception as e:
            inter_summaries.append("")

    # 2) merge summaries and summarize again if needed
    merged = "\n".join([s for s in inter_summaries if s]).strip()
    if not merged:
        merged = text[:3500]

    try:
        out2 = pipe(merged, max_length=max_length, min_length=max(30, min_length // 2), do_sample=False)
        final_sum = out2[0]["summary_text"].strip()
    except Exception:
        final_sum = merged

    # 3) performance-centric scoring (reuse local heuristics)
    perf_flag = "yes" if any(k in final_sum.lower() for k in ["ì„±ê³¼","ë§¤ì¶œ","ì „í™˜","ì§€í‘œ","okr","kpi","kr","íš¨ìœ¨","ì„íŒ©íŠ¸"]) else "no"
    if any(k in final_sum.lower() for k in ["í° ì„íŒ©íŠ¸","ë§¤ì¶œ","ì „í™˜ìœ¨","%","ë‹¬ì„±","ëª©í‘œ ë‹¬ì„±","ì§€í‘œ ê°œì„ "]):
        importance = 4 if perf_flag == "yes" else 2
        if any(k in final_sum.lower() for k in ["ì „ì‚¬", "í•µì‹¬", "ëŒ€ê·œëª¨", "í¬ë¦¬í‹°ì»¬", "ì¥ì•  í•´ê²°", "ì¶œì‹œ"]):
            importance = 5
    else:
        importance = 3 if perf_flag == "yes" else 1

    # 4) split into 5 lines
    # prefer sentence-like splits; fallback to newline split
    parts = re.split(r'(?<=[.!?])\s+|\n+', final_sum)
    parts = [p.strip(" \t\n\râ€¢-") for p in parts if p and len(p.strip()) > 2]
    summary_5 = (parts + [""]*5)[:5]

    # length guard per line
    summary_5 = [(s[:240] + "â€¦") if len(s) > 240 else s for s in summary_5]

    return {"perf_flag": perf_flag, "importance": importance, "summary_5lines": summary_5}

# -------------------- CSV I/O --------------------
def ensure_header(path: str):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    if not os.path.exists(path) or os.path.getsize(path) == 0:
        with open(path, "w", newline="", encoding="utf-8") as f:
            w = csv.writer(f)
            w.writerow(["created_date", "title", "id", "url", "perf_flag", "importance", "summary_5lines"])

def read_input(path: str) -> List[dict]:
    rows = []
    with open(path, "r", newline="", encoding="utf-8") as f:
        rdr = csv.DictReader(f)
        for r in rdr:
            rows.append({
                "created_date": (r.get("created_date") or "").strip(),
                "title": (r.get("title") or "").strip(),
                "id": (r.get("id") or "").strip(),
                "url": (r.get("url") or "").strip(),
            })
    return rows

def load_done_ids(path: str) -> Dict[str, dict]:
    done = {}
    if not os.path.exists(path) or os.path.getsize(path) == 0:
        return done
    with open(path, "r", newline="", encoding="utf-8") as f:
        rdr = csv.DictReader(f)
        for r in rdr:
            rid = (r.get("id") or "").strip()
            if not rid:
                continue
            done[rid] = {
                "created_date": (r.get("created_date") or "").strip(),
                "title": (r.get("title") or "").strip(),
                "url": (r.get("url") or "").strip(),
                "perf_flag": (r.get("perf_flag") or "").strip() or "no",
                "importance": (r.get("importance") or "").strip() or "1",
                "summary_5lines": (r.get("summary_5lines") or "").strip(),
            }
    return done

def append_result(path: str, rows: List[dict]):
    if not rows:
        return
    ensure_header(path)
    with open(path, "a", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        for r in rows:
            w.writerow([
                r["created_date"],
                r["title"],
                r["id"],
                r["url"],
                r["perf_flag"],
                r["importance"],
                r["summary_5lines"],
            ])

# -------------------- ë©”ì¸ --------------------
def main():
    import argparse

    parser = argparse.ArgumentParser(description="manifest_dedup_sorted.csv â†’ AI enriched CSV")
    parser.add_argument("--input", required=True, help="ì…ë ¥ CSV (manifest_dedup_sorted.csv)")
    parser.add_argument("--output", required=True, help="ì¶œë ¥ CSV (manifest_ai_enriched.csv)")

    # ì¬ì‹¤í–‰/ê±´ë„ˆë›°ê¸° ì •ì±…
    parser.add_argument("--resume", action="store_true", default=False, help="ê¸°ì¡´ ì¶œë ¥ì—ì„œ ì´ì–´ì„œ ì§„í–‰")
    parser.add_argument("--overwrite", action="store_true", default=False, help="ì¶œë ¥ íŒŒì¼ì„ ìƒˆë¡œ ìƒì„±(ê¸°ì¡´ ë¬´ì‹œ)")
    parser.add_argument("--force", "--force-retry-failed", dest="force_retry_failed",
                        action="store_true", default=False,
                        help="ì´ì „ ì‹¤íŒ¨(ê¸°ë³¸ìš”ì•½/ë¹ˆìš”ì•½ í¬í•¨) ê±´ ì¬ì‹œë„")
    parser.add_argument("--only-missing", dest="only_missing",
                        action="store_true", default=False,
                        help="ê¸°ì¡´ ì¶œë ¥ì— ì—†ëŠ” idë§Œ ì²˜ë¦¬")

    # OpenAI ì˜µì…˜
    parser.add_argument("--use-openai", action="store_true", default=False,
                        help="OpenAIë¡œ ìš”ì•½/ìŠ¤ì½”ì–´ ìƒì„±")
    parser.add_argument("--openai-model", type=str, default="gpt-4o-mini",
                        help="OpenAI ëª¨ë¸ëª… (ê¸°ë³¸ gpt-4o-mini)")
    parser.add_argument("--openai-timeout", type=int, default=60,
                        help="OpenAI í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ(ì´ˆ)")
    parser.add_argument("--openai-debug", action="store_true", default=False,
                        help="OpenAI ì‘ë‹µ ì›ë¬¸ ì¼ë¶€ ë¡œê·¸")
    parser.add_argument("--openai-sanity-check", action="store_true", default=False,
                        help="ì‹œì‘ ì‹œ 1íšŒ í…ŒìŠ¤íŠ¸ ì½œ í›„ ì§„í–‰")
    parser.add_argument("--openai-max-retries", type=int, default=8, help="OpenAI ì¬ì‹œë„ ìµœëŒ€ íšŸìˆ˜")
    parser.add_argument("--openai-initial-backoff", type=float, default=1.0, help="OpenAI ì¬ì‹œë„ ì´ˆê¸° ëŒ€ê¸°(ì´ˆ)")
    parser.add_argument("--openai-max-backoff", type=float, default=60.0, help="OpenAI ì¬ì‹œë„ ìµœëŒ€ ëŒ€ê¸°(ì´ˆ)")
    parser.add_argument("--min-ai-interval", type=float, default=0.0, help="OpenAI í˜¸ì¶œ ìµœì†Œ ê°„ê²©(ì´ˆ) - ë ˆì´íŠ¸ë¦¬ë°‹ íšŒí”¼")

    # Hugging Face ì˜µì…˜
    parser.add_argument("--use-hf", action="store_true", default=False,
                        help="Hugging Face transformers ìš”ì•½ ì‚¬ìš© (OpenAI ëŒ€ì²´)")
    parser.add_argument("--hf-model", type=str, default="facebook/bart-large-cnn",
                        help="HF summarization ëª¨ë¸ëª… (ex. facebook/bart-large-cnn, google/pegasus-xsum)")
    parser.add_argument("--hf-max-length", type=int, default=220,
                        help="HF ìš”ì•½ max_length")
    parser.add_argument("--hf-min-length", type=int, default=60,
                        help="HF ìš”ì•½ min_length")

    # ì§„í–‰/ì„±ëŠ¥ ì˜µì…˜
    parser.add_argument("--max-blocks", type=int, default=2000, help="ë³¸ë¬¸ ìŠ¤ìº” ìµœëŒ€ ë¸”ë¡ ìˆ˜")
    parser.add_argument("--autosave-every", type=int, default=200, help="Nê±´ë§ˆë‹¤ ìë™ ì €ì¥")
    parser.add_argument("--progress-every", type=int, default=50, help="Nê±´ë§ˆë‹¤ ì§„í–‰ ë¡œê·¸")
    parser.add_argument("--per-item-log", action="store_true", default=False, help="ê° í–‰ ì‹œì‘ ë¡œê·¸")

    args = parser.parse_args()

    # Notion í† í° ê²€ì¦(ìš”ì•½ì— í•„ìš”í•œ ë³¸ë¬¸ì„ ë¶ˆëŸ¬ì˜¤ë ¤ë©´ í•„ìš”)
    if not API_TOKEN or API_TOKEN == "PUT_YOUR_INTEGRATION_TOKEN_HERE":
        log("â— NOTION_TOKEN í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ ìŠ¤í¬ë¦½íŠ¸ ìƒë‹¨ API_TOKENì„ ì‹¤ì œ í† í°ìœ¼ë¡œ ë°”ê¾¸ì„¸ìš”.")
        sys.exit(2)
    validate_token_ascii(API_TOKEN)

    # OpenAI í‚¤ í™•ì¸
    if args.use_openai and not os.environ.get("OPENAI_API_KEY"):
        log("â— OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤. --use-openai ë¥¼ ë„ê±°ë‚˜ í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        sys.exit(2)

    # Ctrl+C ì•ˆì „ ì¢…ë£Œ
    stop_flag = {"stop": False}
    def on_sigint(signum, frame):
        stop_flag["stop"] = True
        log("\nì‚¬ìš©ì ì¤‘ë‹¨: í˜„ì¬ê¹Œì§€ ì§„í–‰ë¶„ ì €ì¥ ì¤‘â€¦")
    signal.signal(signal.SIGINT, on_sigint)

    # ì…ë ¥/ì¶œë ¥ ì¤€ë¹„
    in_rows = read_input(args.input)

    if args.overwrite and os.path.exists(args.output):
        os.remove(args.output)
    ensure_header(args.output)

    done_map = {}
    if args.resume and os.path.exists(args.output):
        done_map = load_done_ids(args.output)
        log(f"ì¬ê°œ ëª¨ë“œ: ê¸°ì¡´ ì¶œë ¥ {len(done_map)}ê±´ ë¡œë“œ")

    # OpenAI ìƒŒí‹°í‹° ì²´í¬(ì˜µì…˜)
    if args.use_openai and args.openai_sanity_check:
        try:
            _ = call_openai_summary(
                "This is a sanity check.",
                model=args.openai_model,
                timeout=args.openai_timeout,
                debug=args.openai_debug,
                max_retries=args.openai_max_retries,
                initial_backoff=args.openai_initial_backoff,
                max_backoff=args.openai_max_backoff,
                min_interval=args.min_ai_interval,
            )
            log("[OpenAI] Sanity check OK")
        except KeyboardInterrupt:
            raise
        except Exception as e:
            msg = f"[OpenAI] Sanity check FAILED({type(e).__name__}): {e}"
            log(msg)
            append_failure(FAIL_LOG, msg)
            # 'insufficient_quota' ë“±ì€ ìë™ í´ë°±
            log("â†’ OpenAI ìš”ì•½ì„ ë¹„í™œì„±í™”í•˜ê³  ê¸°ë³¸ ìš”ì•½ìœ¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤. (ë‚˜ì¤‘ì— --force ë¡œ ì¬ì‹œë„ ê°€ëŠ¥)")
            args.use_openai = False
            if getattr(args, "use_hf", False):
                log("â†’ ì´í›„ ëª¨ë“  í•­ëª©ì€ HF ìš”ì•½ê¸°ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            else:
                log("â†’ ì´í›„ ëª¨ë“  í•­ëª©ì€ ë¡œì»¬ ìš”ì•½ê¸°ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        
    total = len(in_rows)
    # ì²˜ë¦¬ ëŒ€ìƒ íŒë³„
    def is_failed_row(v: dict) -> bool:
        s = (v.get("summary_5lines") or "").strip()
        return (not s) or ("AI ìš”ì•½ ì‹¤íŒ¨" in s)

    targets = []
    for r in in_rows:
        rid = r["id"]
        if not rid:
            continue
        if args.only_missing:
            if rid in done_map:
                continue
        elif args.force_retry_failed:
            if rid in done_map and is_failed_row(done_map[rid]):
                pass  # ì¬ì²˜ë¦¬
            elif rid in done_map:
                continue  # ì„±ê³µê±´ì€ ìŠ¤í‚µ
        else:
            if rid in done_map:
                continue
        targets.append(r)

    log(f"ì‹œì‘: ì „ì²´ {total}ê±´ / ì²˜ë¦¬ëŒ€ìƒ {len(targets)}ê±´ "
        f"(resume={'ON' if args.resume else 'OFF'}, overwrite={'ON' if args.overwrite else 'OFF'})")

    processed = 0
    written_since_autosave = 0
    pending_rows: List[dict] = []

    def flush(reason: str):
        nonlocal pending_rows, written_since_autosave
        if pending_rows:
            append_result(args.output, pending_rows)
            pending_rows.clear()
            written_since_autosave = 0
        log(f"[ì €ì¥] {reason} â†’ {args.output}")

    for idx, r in enumerate(targets, start=1):
        if stop_flag["stop"]:
            break

        created_date = r["created_date"]
        title = r["title"]
        nid = r["id"]
        url = r["url"]

        if args.per_item_log:
            log(f"[{idx}/{len(targets)}] start id={nid} title={title}")

        # ë³¸ë¬¸ ìˆ˜ì§‘
        md = ""
        try:
            page = retrieve_page(nid)
            if page:
                md = get_page_markdown(nid, max_blocks=args.max_blocks)
        except KeyboardInterrupt:
            raise
        except Exception as e:
            append_failure(FAIL_LOG, f"ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {type(e).__name__}: {e}", {"id": nid})

        # ìš”ì•½ ê²½ë¡œ ì„ íƒ: OpenAI â†’ HF â†’ ë¡œì»¬ íœ´ë¦¬ìŠ¤í‹±
        if args.use_openai:
            try:
                ai = call_openai_summary(
                    md[:80000],  # ê³¼ë„í•œ ê¸¸ì´ ë°©ì§€
                    model=args.openai_model,
                    timeout=args.openai_timeout,
                    debug=args.openai_debug,
                    max_retries=args.openai_max_retries,
                    initial_backoff=args.openai_initial_backoff,
                    max_backoff=args.openai_max_backoff,
                    min_interval=args.min_ai_interval,
                )
                perf_flag = ai["perf_flag"]
                importance = ai["importance"]
                summary_lines = ai["summary_5lines"]
                log(f"[OPENAI] ìš”ì•½ ì„±ê³µ id={nid} imp={importance} perf={perf_flag}")
            except KeyboardInterrupt:
                raise
            except Exception as e:
                print(f"[AI-ERROR] {type(e).__name__}: {e}")
                append_failure(FAIL_LOG, f"OpenAI ì‹¤íŒ¨: {type(e).__name__}: {e}", {"id": nid})
                # OpenAI ì‹¤íŒ¨ ì‹œ HFê°€ ì¼œì ¸ ìˆìœ¼ë©´ HFë¡œ í´ë°±
                if getattr(args, "use_hf", False):
                    try:
                        ai = summarize_with_hf(
                            md[:40000],
                            model_name=args.hf_model,
                            max_length=args.hf_max_length,
                            min_length=args.hf_min_length,
                        )
                        perf_flag = ai["perf_flag"]
                        importance = ai["importance"]
                        summary_lines = ai["summary_5lines"]
                        log(f"[HF-FB] OpenAI ì‹¤íŒ¨ â†’ HF ìš”ì•½ ì‚¬ìš© id={nid} imp={importance} perf={perf_flag}")
                    except Exception as e2:
                        append_failure(FAIL_LOG, f"HF ì‹¤íŒ¨: {type(e2).__name__}: {e2}", {"id": nid})
                        local = summarize_locally(md, title, created_date, url)
                        perf_flag = local["perf_flag"]
                        importance = local["importance"]
                        summary_lines = local["summary_5lines"]
                        log(f"[LOCAL-FB] HF ì‹¤íŒ¨ â†’ ë¡œì»¬ ìš”ì•½ ì‚¬ìš© id={nid} imp={importance} perf={perf_flag}")
                else:
                    local = summarize_locally(md, title, created_date, url)
                    perf_flag = local["perf_flag"]
                    importance = local["importance"]
                    summary_lines = local["summary_5lines"]
                    log(f"[LOCAL-FB] OpenAI ì‹¤íŒ¨ â†’ ë¡œì»¬ ìš”ì•½ ì‚¬ìš© id={nid} imp={importance} perf={perf_flag}")
        elif getattr(args, "use_hf", False):
            try:
                ai = summarize_with_hf(
                    md[:40000],
                    model_name=args.hf_model,
                    max_length=args.hf_max_length,
                    min_length=args.hf_min_length,
                )
                perf_flag = ai["perf_flag"]
                importance = ai["importance"]
                summary_lines = ai["summary_5lines"]
                log(f"[HF] ìš”ì•½ ì‚¬ìš© id={nid} imp={importance} perf={perf_flag}")
            except Exception as e:
                append_failure(FAIL_LOG, f"HF ì‹¤íŒ¨: {type(e).__name__}: {e}", {"id": nid})
                local = summarize_locally(md, title, created_date, url)
                perf_flag = local["perf_flag"]
                importance = local["importance"]
                summary_lines = local["summary_5lines"]
                log(f"[LOCAL-FB] HF ì‹¤íŒ¨ â†’ ë¡œì»¬ ìš”ì•½ ì‚¬ìš© id={nid} imp={importance} perf={perf_flag}")
        else:
            # ë¡œì»¬ íœ´ë¦¬ìŠ¤í‹± ìš”ì•½
            local = summarize_locally(md, title, created_date, url)
            perf_flag = local["perf_flag"]
            importance = local["importance"]
            summary_lines = local["summary_5lines"]
            log(f"[LOCAL] ìš”ì•½ ì‚¬ìš© id={nid} imp={importance} perf={perf_flag}")

        # ê²°ê³¼ ì ì¬
        out_row = {
            "created_date": created_date,
            "title": title,
            "id": nid,
            "url": url,
            "perf_flag": perf_flag,
            "importance": str(importance),
            "summary_5lines": "\n".join(summary_lines),
        }
        pending_rows.append(out_row)
        written_since_autosave += 1
        processed += 1

        # ì§„í–‰ ë¡œê·¸/ìë™ ì €ì¥
        if args.progress_every and (processed % args.progress_every == 0):
            log(f"[ì§„í–‰] {processed}/{len(targets)}")
        if args.autosave_every and (written_since_autosave >= args.autosave_every):
            flush("ìë™ ì €ì¥")

    # ë§ˆì§€ë§‰ ì €ì¥
    flush("ìµœì¢… ì €ì¥")
    log(f"\nì™„ë£Œ âœ… ê²°ê³¼ ì €ì¥: {args.output}")
    log(f"ì´ ì²˜ë¦¬: {processed}/{len(targets)}")

if __name__ == "__main__":
    try:
        main()
    except SystemExit:
        raise
    except KeyboardInterrupt:
        log("\nì‚¬ìš©ì ì¤‘ë‹¨(KeyboardInterrupt)")
        raise
    except Exception as e:
        append_failure(FAIL_LOG, f"ì¹˜ëª…ì  ì˜¤ë¥˜: {type(e).__name__}: {e}")
        raise