#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
export_manifest_ai_fields.py
- ì…ë ¥: manifest_dedup_sorted.csv (created_date,title,id,url)
- ì¶œë ¥: manifest_ai_enriched.csv (created_date,title,id,url,perf_flag,importance,summary_5lines)
- ê¸°ëŠ¥:
  * Notion í˜ì´ì§€ ë³¸ë¬¸ì„ Markdownìœ¼ë¡œ ìˆ˜ì§‘(ë¸”ë¡ í˜ì´ì§€ë„¤ì´ì…˜)
  * OpenAIë¡œ 'ì„±ê³¼ì— ì“¸ë§Œí•œì§€' ê¸°ì¤€ 5ì¤„ ìš”ì•½ + ì¤‘ìš”ë„(1~5) + perf_flag(yes/no)
  * --resume: ê¸°ì¡´ ì¶œë ¥ì´ ìˆìœ¼ë©´ ì´ì–´ì„œ(ì´ë¯¸ ì²˜ë¦¬í•œ idëŠ” ìŠ¤í‚µ)
  * --force / --force-retry-failed / --only-missing ë¡œ ì¬ì²˜ë¦¬ ì„ íƒ
  * ì§„í–‰ ë¡œê·¸(í–‰ ë‹¨ìœ„/êµ¬ê°„ ë‹¨ìœ„), ìë™ ì €ì¥, ì˜ˆì™¸ ìƒì„¸ ë¡œê·¸
  * latin-1 í—¤ë” ë³´í˜¸(í† í°ì— ë¹„ASCIIê°€ ì„ì¼ ë•Œ urllib3 UnicodeEncodeError ë°©ì§€)
"""

import os
import sys
import csv
import json
import time
import signal
import re
from typing import Dict, List, Set, Optional, Tuple
from datetime import datetime
import shutil

import requests
import subprocess
import getpass

# -------------------- .env ë¡œë” --------------------
def _load_dotenv(paths: Optional[List[str]] = None, override: bool = True) -> None:
    """
    ê°„ë‹¨í•œ .env ë¡œë”: key=value / export key=value / ë”°ì˜´í‘œ ê°’ ì§€ì›.
    - ê¸°ë³¸ ê²€ìƒ‰ ê²½ë¡œ: ./.env, ./.env.local, ~/.env
    - override=Trueì´ë©´ ê¸°ì¡´ í™˜ê²½ê°’ ìœ„ì— ë®ì–´ì”€.
    ë³´ì•ˆìƒ ê°’ì€ ë¡œê·¸ì— ì¶œë ¥í•˜ì§€ ì•ŠìŒ.
    """
    try:
        if paths is None:
            paths = [
                os.path.join(os.getcwd(), ".env"),
                os.path.join(os.getcwd(), ".env.local"),
                os.path.expanduser("~/.env"),
            ]
        for p in paths:
            if not p or not os.path.exists(p):
                continue
            try:
                with open(p, "r", encoding="utf-8") as f:
                    for line in f:
                        s = line.strip()
                        if not s or s.startswith("#"):
                            continue
                        if s.lower().startswith("export "):
                            s = s[7:].lstrip()
                        if "=" not in s:
                            continue
                        k, v = s.split("=", 1)
                        k = k.strip()
                        v = v.strip()
                        # ë”°ì˜´í‘œ ì œê±°
                        if (v.startswith('"') and v.endswith('"')) or (v.startswith("'") and v.endswith("'")):
                            v = v[1:-1]
                        # ê°„ë‹¨í•œ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
                        v = v.replace("\\n", "\n").replace("\\t", "\t")
                        if override or not os.environ.get(k):
                            os.environ[k] = v
            except Exception:
                # .env íŒŒì‹± ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•ŠìŒ
                pass
    except Exception:
        pass

# .env ë¡œë“œ: API_TOKEN ê³„ì‚° ì „ì— ì‹¤í–‰ë˜ì–´ì•¼ í•¨
_load_dotenv()

# -------------------- macOS Keychain/í”„ë¡¬í”„íŠ¸ ë¹„ë°€ ë¡œë” --------------------
def _get_secret(name: str, prompt_label: str) -> str:
    """
    ìš°ì„ ìˆœìœ„: í™˜ê²½ë³€ìˆ˜ â†’ macOS Keychain(security) â†’ í„°ë¯¸ë„ ë¹„ë°€ ì…ë ¥(getpass)
    Keychainì— ì—†ê³  ì‚¬ìš©ìê°€ ì…ë ¥í•˜ë©´, ê°™ì€ ì´ë¦„(Service=name, Account=$USER)ìœ¼ë¡œ ì €ì¥ ì‹œë„.
    """
    val = os.environ.get(name, "").strip()
    if val:
        return val
    # macOS Keychain ì¡°íšŒ
    try:
        user = os.environ.get("USER", "")
        r = subprocess.run(
            ["/usr/bin/security", "find-generic-password", "-a", user, "-s", name, "-w"],
            capture_output=True, text=True
        )
        if r.returncode == 0:
            return r.stdout.strip()
    except Exception:
        pass
    # í„°ë¯¸ë„ ì…ë ¥
    try:
        v = getpass.getpass(f"Enter {prompt_label} (input hidden): ").strip()
        if v:
            try:
                user = os.environ.get("USER", "")
                subprocess.run(
                    ["/usr/bin/security", "add-generic-password", "-U", "-a", user, "-s", name, "-w", v],
                    check=False,
                )
            except Exception:
                pass
            return v
    except Exception:
        pass
    return ""

def _resolve_tokens_from_keychain_or_prompt():
    """NOTION_TOKEN / OPENAI_API_KEY ë³´ì™„ ë¡œë“œ"""
    global API_TOKEN
    if (not API_TOKEN) or API_TOKEN == "PUT_YOUR_INTEGRATION_TOKEN_HERE":
        api = _get_secret("NOTION_TOKEN", "NOTION_TOKEN (Notion integration token)")
        if api:
            API_TOKEN = api
    if not os.environ.get("OPENAI_API_KEY", "").strip():
        v = _get_secret("OPENAI_API_KEY", "OPENAI_API_KEY")
        if v:
            os.environ["OPENAI_API_KEY"] = v

# -------------------- í™˜ê²½/ì„¤ì • --------------------
BASE_URL = "https://api.notion.com/v1"
NOTION_VERSION = "2022-06-28"
API_TOKEN = os.environ.get("NOTION_TOKEN", "").strip() or "PUT_YOUR_INTEGRATION_TOKEN_HERE"

OUT_DIR = "./out"
FAIL_LOG = os.path.join(OUT_DIR, "failures_index.txt")

_session = requests.Session()
_session.trust_env = True

# OpenAI í˜¸ì¶œ ê°„ ìµœì†Œ ê°„ê²©(Throttle) ê´€ë¦¬
_last_ai_ts: float = 0.0

def _throttle(min_interval: float):
    global _last_ai_ts
    if min_interval <= 0:
        return
    now = time.time()
    wait = (_last_ai_ts + min_interval) - now
    if wait > 0:
        time.sleep(wait)
    _last_ai_ts = time.time()

# -------------------- ê³µìš© ìœ í‹¸ --------------------
def log(msg: str = ""):
    try:
        print(msg, flush=True)
    except Exception:
        print(str(msg).encode("ascii", "replace").decode("ascii"), flush=True)

def append_failure(path: str, msg: str, context: Optional[dict] = None):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "a", encoding="utf-8") as f:
        ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        line = {"ts": ts, "message": msg}
        if context:
            line["context"] = context
        f.write(json.dumps(line, ensure_ascii=False) + "\n")

def validate_token_ascii(token: str):
    try:
        _ = token.encode("latin-1")
    except Exception:
        log("â— HTTP í—¤ë” 'Authorization'ì— ë¹„-ASCII ë¬¸ìê°€ ì„ì˜€ìŠµë‹ˆë‹¤. NOTION_TOKEN ê°’ì„ ë‹¤ì‹œ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.")
        sys.exit(2)

def headers() -> Dict[str, str]:
    return {
        "Authorization": f"Bearer {API_TOKEN}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json",
    }

# -------------------- Notion API --------------------
def req(method: str, url: str, **kwargs):
    tries = 0
    while True:
        tries += 1
        try:
            r = _session.request(method, url, headers=headers(), timeout=60, **kwargs)
            if r.status_code in (429, 502, 503):
                time.sleep(min(1.0 * tries, 5))
                continue
            return r
        except Exception as e:
            if tries < 3:
                time.sleep(0.5 * tries)
                continue
            raise

def retrieve_page(page_id: str) -> Optional[dict]:
    r = req("GET", f"{BASE_URL}/pages/{page_id}")
    if r.status_code != 200:
        append_failure(FAIL_LOG, f"/pages/{page_id} ì‹¤íŒ¨ status={r.status_code}")
        return None
    return r.json()

def blocks_children(parent_id: str, start_cursor: Optional[str] = None, page_size: int = 100) -> dict:
    params = {"page_size": min(100, page_size)}
    if start_cursor:
        params["start_cursor"] = start_cursor
    r = req("GET", f"{BASE_URL}/blocks/{parent_id}/children", params=params)
    if r.status_code != 200:
        # 404ëŠ” ì‚­ì œ/ê¶Œí•œ/ì˜ëª»ëœID ë“±
        append_failure(FAIL_LOG, f"/blocks/{parent_id}/children ì‹¤íŒ¨ status={r.status_code}")
        return {"results": [], "has_more": False}
    return r.json()

# -------------------- Markdown ë¹Œë“œ --------------------
# ---- ë¡œì»¬ ìš”ì•½ê¸°(Heuristic Summarizer) ----
import math

_METRIC_PAT = re.compile(r"(\d{1,3}(?:[,\d]{0,3})+(?:\.\d+)?%?)|(%|\bp\d+\b|\bkr\d+\b)", re.I)
_KEYWORDS = [
    "ì„±ê³¼","ì§€í‘œ","ë°ì´í„°","ì „í™˜","ì „í™˜ìœ¨","ë§¤ì¶œ","êµ¬ë§¤","ê°€ì…","ì”ì¡´","ë¦¬í…ì…˜","KPI","OKR","KR",
    "ë‹¬ì„±","ê°œì„ ","ì¦ê°€","ê°ì†Œ","ìƒìŠ¹","í•˜ë½","ì¶œì‹œ","ë°°í¬","ëŸ°ì¹­","ë„ì…","ì‹¤í—˜","A/B","AB","ê°€ì„¤",
    "ê²°ì •","í•©ì˜","ìš°ì„ ìˆœìœ„","ë¦¬íŒ©í„°","ë¦¬íŒ©í† ë§","ë¹„ìš©","íš¨ìœ¨","ì†ë„","ì„±ëŠ¥","ë²„ê·¸","ì¥ì• ","í•´ê²°",
    "íš¨ê³¼","ì„íŒ©íŠ¸","ì˜í–¥","ëª©í‘œ","ê²°ê³¼","Outcome","Impact"
]

_NEGATIVE_HINTS = [
    "íšŒì˜ë¡","ì•ˆê±´","ëª©ì°¨","í…œí”Œë¦¿","ì‘ì„±ìš”ë ¹","ì˜ˆì‹œ","ìƒ˜í”Œ","TODO","To Do","to-do","ì°¸ê³ ","ë§í¬","ë§í¬ëª¨ìŒ",
    "ì œëª©ì—†ìŒ","ë¹ˆ í˜ì´ì§€","ë¹ˆí˜ì´ì§€","ë¯¸ì •","TBD","N/A"
]

# ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ (ì„±ê³¼ ê´€ì  ê°€ì¤‘ì¹˜) - ì‚¬ìš©ìê°€ ì§€ì •í•œ ë‹¨ì–´ ê¸°ë°˜
# ê°€ì¤‘ì¹˜ ê¸°ì¤€(ê¶Œì¥): 1=ë‚®ìŒ, 2=ë³´í†µ, 3=ë†’ìŒ
PRIORITY_KEYWORDS = {
    "ì„±ê³¼": 3,
    "ì „í™˜ìœ¨": 3,
    "í¼ë„": 3,
    "ë°°í¬": 3,
    "roas": 3,       # ëŒ€ì†Œë¬¸ì ë¬´ì‹œ
    "abt": 3,        # ABT (A/B í…ŒìŠ¤íŠ¸ ê´€ë ¨)
    "a/b": 3,        # 'A/B' í‘œê¸°ë„ ë³´ê°•
    "ì‹¤í—˜": 3,
    "ë°°í¬ ê³µìœ ": 2,
    "íš¨ìœ¨í™”": 2,
    "ìë™í™”": 2,
    "prd": 2,
    "ë¬¸ì œì •ì˜": 2,
    "ë¬¸ì œ ì •ì˜": 2,
    "ë¬¸ì œ": 2,
    "ì§€í‘œ": 1,
    "ì„±ê³µ": 1,
    "ì‹¤íŒ¨": 1,
    "ìš”ì•½": 1,
}

def _clean_line(s: str) -> str:
    s = s.strip()
    # ë§ˆí¬ë‹¤ìš´ ë¨¸ë¦¬ê¸°í˜¸ ì œê±°
    s = re.sub(r"^#{1,6}\s*", "", s)   # headings
    s = re.sub(r"^[-*+]\s+", "", s)    # bullets
    s = re.sub(r"^\d+\.\s+", "", s)    # ordered list
    s = re.sub(r"^>\s*", "", s)        # quote
    s = s.replace("[x]", "").replace("[ ]", "")
    # ì½”ë“œ/êµ¬ë¶„ì„ /ë¹ˆ ì¤„ ì œê±°
    if s.startswith("```") or s == "---":
        return ""
    return s.strip()

def _score_line(s: str) -> int:
    if not s or len(s) < 4:
        return 0
    score = 0
    # í•µì‹¬ í‚¤ì›Œë“œ
    for kw in _KEYWORDS:
        if kw.lower() in s.lower():
            score += 2
            break
    # ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ (ì—¬ëŸ¬ ê°œ í¬í•¨ ì‹œ ëˆ„ì )
    low = s.lower()
    for kw, weight in PRIORITY_KEYWORDS.items():
        if kw in low:
            score += weight * 3  # ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œëŠ” ê°•í•˜ê²Œ ë°˜ì˜
    # ìˆ˜ì¹˜/í¼ì„¼íŠ¸/ì§€í‘œ
    if _METRIC_PAT.search(s):
        score += 2
    # ê°•í•œ ë™ì‚¬
    for v in ["ë‹¬ì„±","ê°œì„ ","ì¦ê°€","ê°ì†Œ","ê°ì†Œì‹œí‚´","ìƒìŠ¹","ì¶œì‹œ","ë°°í¬","ë„ì…","í•´ê²°","ì„±ê³¼","íš¨ìœ¨"]:
        if v in s:
            score += 1
            break
    # ê²°ì •/ì •ì±…/ìš°ì„ ìˆœìœ„
    for v in ["ê²°ì •","í•©ì˜","ì •ì±…","ê°€ì´ë“œ","ìš°ì„ ìˆœìœ„","ì¤‘ìš”", "í•µì‹¬"]:
        if v in s:
            score += 1
            break
    # ë¶€ì •ì /í˜•ì‹ì  ë¼ì¸ ê°ì 
    for ng in _NEGATIVE_HINTS:
        if ng.lower() in s.lower():
            score -= 1
            break
    # ë„ˆë¬´ ì§§ê±°ë‚˜ ë„ˆë¬´ ê¸¸ë©´ ì•½ê°„ ê°ì /ë³´ì •
    if len(s) > 220:
        score -= 1
    return max(score, 0)

def summarize_locally(markdown_text: str, title: str, created_date: str, url: str) -> dict:
    """
    ë§ˆí¬ë‹¤ìš´ì„ ê°€ë³ê²Œ ìŠ¤ìº”í•˜ì—¬ 'ì„±ê³¼ ê´€ì ' 5ì¤„ ìš”ì•½, perf_flag, importanceë¥¼ ìƒì„±.
    - í›„ë³´: í—¤ë”©/ë¦¬ìŠ¤íŠ¸/ìˆ«ìí¬í•¨/í‚¤ì›Œë“œ í¬í•¨ ë¼ì¸ ìœ„ì£¼
    - ì ìˆ˜: í‚¤ì›Œë“œ/ìˆ«ì/ë™ì‚¬/ê²°ì • í‚¤ì›Œë“œë¡œ ê°€ì¤‘
    """
    def _is_noise(s: str) -> bool:
        if not s:
            return True
        punct = sum(1 for ch in s if ch in "`~!@#$%^&*()_-+=|\\{}[]:;\"'<>,.?/Â·â€¢â—â—‹â˜…â˜†â–³â–·Â©Â®â„¢ï¼Œã€‚ï¼›ï¼šï¼ï¼Ÿã€ã€ã€â€œâ€â€™â€â€œâ€¦ï¼…â€°")
        if punct / max(1, len(s)) > 0.35:
            return True
        core = sum(1 for ch in s if ("0" <= ch <= "9") or ("a" <= ch.lower() <= "z") or ("ê°€" <= ch <= "í£"))
        if core / max(1, len(s)) < 0.4 and len(s) < 50:
            return True
        if len(s.split()) <= 1 and len(s) < 8:
            return True
        return False

    lines = []
    for raw in (markdown_text or "").splitlines():
        s = _clean_line(raw)
        if not s:
            continue
        # ì˜ë¯¸ ì—†ëŠ” ë¼ì¸ í•„í„°
        if s.lower().startswith(("http://","https://")):
            continue
        if _is_noise(s):
            continue
        # callout ì´ëª¨ì§€ ì œê±° í”ì 
        s = s.replace("ğŸ’¡", "").strip()
        lines.append(s)

    # í›„ë³´ ì„ ë³„
    candidates = []
    for s in lines:
        base = 0
        if any(s.startswith(prefix) for prefix in ("#", "-", "1.", "2.", "3.", "OKR", "KR", "[", "â€¢", "Â·")):
            base += 1
        if any(kw.lower() in s.lower() for kw in _KEYWORDS) or _METRIC_PAT.search(s):
            base += 1
        score = _score_line(s) + base
        if score > 0:
            candidates.append((score, s))

    # ì¤‘ë³µ/ìœ ì‚¬ ë¬¸ì¥ ì œê±° ê°„ë‹¨ ì²˜ë¦¬
    seen = set()
    deduped = []
    def _priority_hit(t: str) -> int:
        tl = t.lower()
        return 1 if any(pk in tl for pk in PRIORITY_KEYWORDS.keys()) else 0

    for score, s in sorted(candidates, key=lambda x: (-x[0], -_priority_hit(x[1]), len(x[1]))):
        key = s.lower()
        if key in seen:
            continue
        seen.add(key)
        deduped.append((score, s))
        if len(deduped) >= 80:
            break

    # ìµœê³  ì ìˆ˜ ê¸°ë°˜ perf/importance ì‚°ì¶œ (ë¶„í¬ë¥¼ ë” ê· ë“±í•˜ê²Œ)
    top_score = deduped[0][0] if deduped else 0
    lowtext = (markdown_text or "").lower()
    perf_flag = "yes" if (top_score >= 3 or any(k in lowtext for k in ["okr","kr","kpi","ì „í™˜","ì „í™˜ìœ¨","ë§¤ì¶œ","roas","ì‹¤í—˜","a/b"])) else "no"

    # ë¶„í¬í˜• íŠ¹ì§•ì¹˜: ìƒìœ„ nê°œ í‰ê· , ë°€ë„, ì§€í‘œ/í‚¤ì›Œë“œ ì¡´ì¬ ì—¬ë¶€
    top_nscores = [sc for sc, _ in deduped[:5]] if deduped else []
    top_avg = sum(top_nscores) / max(1, len(top_nscores))
    dense_cnt = sum(1 for sc, _ in deduped if sc >= 3)
    dense_ratio = dense_cnt / max(1, len(lines) or 1)
    metric_hit = bool(_METRIC_PAT.search(markdown_text or ""))

    # ê¸°ë³¸ ì¤‘ìš”ë„ ê°€ì¤‘ (ê· ë“± ë¶„í¬ ì§€í–¥)
    if perf_flag == "no":
        importance = 1
    else:
        if metric_hit and (top_score >= 6 or dense_ratio >= 0.25):
            importance = 5
        elif (top_avg >= 5) or (dense_ratio >= 0.18) or (top_score >= 5):
            importance = 4
        elif (top_avg >= 3) or (dense_ratio >= 0.10) or metric_hit:
            importance = 3
        elif (top_score >= 2):
            importance = 2
        else:
            importance = 2  # perf=yesì¸ë° ìƒìœ„ ìŠ¤ì½”ì–´ ë‚®ìœ¼ë©´ 2ë¡œ ì™„ì¶©
    importance = int(max(1, min(5, importance)))

    # ìƒìœ„ 5ì¤„ êµ¬ì„±
    top_lines = [s for _, s in deduped][:5]
    if len(top_lines) < 5:
        # ë³´ê°•: ì œëª©/ë‚ ì§œ/ë§í¬ë¡œ ì±„ì›€
        filler = [
            f"[ì œëª©] {title}".strip(),
            f"[ë‚ ì§œ] {created_date}".strip(),
            f"[ë§í¬] {url}".strip()
        ]
        for x in filler:
            if len(top_lines) >= 5: break
            if x and x not in top_lines:
                top_lines.append(x)
    # ì „ë¶€ ë³´ê°•ì„± ë©”íƒ€ë§Œ ë‚¨ëŠ” ê²½ìš°, ì•ˆë‚´ ë¬¸ì¥ìœ¼ë¡œ ëŒ€ì²´
    if sum(1 for t in top_lines if t.startswith("[")) >= 3 and len([t for t in top_lines if t and not t.startswith("[")]) == 0:
        top_lines = [
            "ë³¸ë¬¸ì´ ì§§ê±°ë‚˜ í˜•ì‹ ìœ„ì£¼ë¼ ì„±ê³¼ ìš”ì•½ì´ ì–´ë µìŠµë‹ˆë‹¤.",
        ]
    # ê¸¸ì´ ì œí•œ
    top_lines = [ (l[:240] + "â€¦") if len(l) > 240 else l for l in top_lines ]
    # 5ê°œ ë§ì¶”ê¸°
    top_lines = (top_lines + [""]*5)[:5]

    return {"perf_flag": perf_flag, "importance": importance, "summary_5lines": top_lines}
def _rich_text_to_plain(rt_arr: list) -> str:
    buf = []
    for o in rt_arr or []:
        t = o.get("type")
        if t == "text":
            buf.append(o.get("text", {}).get("content", ""))
        elif t == "mention":
            m = o.get("mention", {}) or {}
            if m.get("type") == "user":
                name = (m.get("user", {}) or {}).get("name", "")
                buf.append(f"@{name}" if name else "@user")
            else:
                buf.append("@mention")
        elif t == "equation":
            buf.append(o.get("equation", {}).get("expression", ""))
    return "".join(buf)

def block_to_md(block: dict) -> str:
    t = block.get("type")
    b = block.get(t) or {}
    if t == "paragraph":
        return _rich_text_to_plain(b.get("rich_text"))
    if t in ("heading_1", "heading_2", "heading_3"):
        hashes = {"heading_1": "#", "heading_2": "##", "heading_3": "###"}[t]
        return f"{hashes} {_rich_text_to_plain(b.get('rich_text'))}"
    if t in ("bulleted_list_item", "numbered_list_item"):
        prefix = "-" if t == "bulleted_list_item" else "1."
        return f"{prefix} {_rich_text_to_plain(b.get('rich_text'))}"
    if t == "to_do":
        chk = "[x]" if b.get("checked") else "[ ]"
        return f"- {chk} {_rich_text_to_plain(b.get('rich_text'))}"
    if t == "quote":
        return f"> {_rich_text_to_plain(b.get('rich_text'))}"
    if t == "callout":
        return f"> ğŸ’¡ {_rich_text_to_plain(b.get('rich_text'))}"
    if t == "code":
        lang = b.get("language") or ""
        txt = _rich_text_to_plain(b.get("rich_text"))
        return f"```{lang}\n{txt}\n```"
    if t == "divider":
        return "---"
    if t == "bookmark":
        url = b.get("url", "")
        cap = _rich_text_to_plain(b.get("caption") or [])
        return f"[{cap or url}]({url})" if url else cap
    if t == "image":
        cap = _rich_text_to_plain(b.get("caption") or [])
        return f"![image]({cap})" if cap else "![image]"
    # child_page/child_database ë“±ì€ ìƒìœ„ì—ì„œ ë³„ë„ë¡œ ìˆœíšŒí•˜ë¯€ë¡œ ìŠ¤í‚µ
    return ""

def get_page_markdown(page_id: str, max_blocks: int = 2000, depth: int = 2) -> str:
    """ì²« max_blocks ë§Œí¼ childrenì„ depth ë‹¨ê³„ê¹Œì§€ Markdownìœ¼ë¡œ ê²°í•©"""
    lines: List[str] = []
    scanned = 0

    def walk(parent_id: str, cur_depth: int, cursor: Optional[str] = None):
        nonlocal scanned, lines
        if scanned >= max_blocks:
            return
        data = blocks_children(parent_id, start_cursor=cursor, page_size=100)
        results = data.get("results", [])
        for blk in results:
            if scanned >= max_blocks:
                break
            md = block_to_md(blk)
            if md:
                lines.append(md)
            scanned += 1
            if cur_depth > 1 and blk.get("has_children"):
                try:
                    walk(blk.get("id"), cur_depth - 1, None)
                except Exception:
                    pass
        if data.get("has_more") and scanned < max_blocks:
            walk(parent_id, cur_depth, data.get("next_cursor"))

    walk(page_id, max(1, int(depth)))
    return "\n".join(lines).strip()

# -------------------- OpenAI --------------------
# pip install openai
from openai import OpenAI
_openai_client = None

def get_openai():
    global _openai_client
    if _openai_client is None:
        _openai_client = OpenAI()  # í‚¤ëŠ” OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ì—ì„œ ìë™ ë¡œë“œ
    return _openai_client

def parse_ai_output(text: str) -> dict:
    """
    ëª¨ë¸ì´ ì•„ë˜ JSONì„ ë°˜í™˜í•œë‹¤ê³  ê°€ì •:
    {
      "perf_flag": "yes"|"no",
      "importance": 1..5,
      "summary_5lines": ["...","...","...","...","..."]
    }
    ì½”ë“œë¸”ë¡ ```json ... ``` í˜•íƒœë„ í—ˆìš©.
    """
    import json as _json, re as _re

    # ì½”ë“œë¸”ë¡ ì œê±°
    code_match = _re.search(r"```(?:json)?(.*?)```", text, _re.S | _re.I)
    if code_match:
        text = code_match.group(1).strip()

    # ì²« ë²ˆì§¸ JSON ì˜¤ë¸Œì íŠ¸ë§Œ ì¶”ì¶œ
    m = _re.search(r"\{.*\}", text, _re.S)
    raw = m.group(0) if m else text.strip()

    # JSON íŒŒì‹± (ì‹¤íŒ¨ ì‹œ ì•ˆì „ ê¸°ë³¸ê°’ ë°˜í™˜)
    try:
        data = _json.loads(raw)
    except Exception:
        return {"perf_flag": "no", "importance": 1, "summary_5lines": ["", "", "", "", ""]}

    perf = str(data.get("perf_flag", "no")).lower()
    perf = "yes" if perf in ("yes", "y", "true", "1") else "no"

    try:
        imp = int(data.get("importance", 1))
    except Exception:
        imp = 1
    imp = min(5, max(1, imp))

    lines = data.get("summary_5lines", [])
    if isinstance(lines, str):
        lines = [s.strip() for s in lines.split("\n") if s.strip()]
    if not isinstance(lines, list):
        lines = []
    lines = (lines + [""] * 5)[:5]

    return {"perf_flag": perf, "importance": imp, "summary_5lines": lines}

def call_openai_summary(
    markdown_text: str,
    model: str,
    timeout: int,
    debug: bool = False,
    max_retries: int = 8,
    initial_backoff: float = 1.0,
    max_backoff: float = 60.0,
    min_interval: float = 0.0,
    summary_lang: str = "ko",
    chunk_threshold: int = 16000,
    chunk_size: int = 6000,
    chunk_overlap: int = 400,
) -> dict:
    """
    ì „ì²´ ë§ˆí¬ë‹¤ìš´ì„ ì½ê³  'ì„±ê³¼ì— ì“¸ë§Œí•œê°€' ê¸°ì¤€ìœ¼ë¡œ 5ì¤„ ìš”ì•½/ì¤‘ìš”ë„/ì„±ê³¼ì—¬ë¶€ë¥¼ JSONìœ¼ë¡œ ë°˜í™˜.
    429/5xx/ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ëŠ” ì§€ìˆ˜ ë°±ì˜¤í”„+ì§€í„°ë¡œ ì¬ì‹œë„. í˜¸ì¶œ ê°„ ìµœì†Œê°„ê²©(min_interval) ë³´ì¥.
    ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ê·¸ëŒ€ë¡œ ë˜ì§(ìƒìœ„ì—ì„œ ê¸°ë³¸ìš”ì•½ ì²˜ë¦¬).
    """
    client = get_openai()

    # --- ë‚´ë¶€: ë‹¨ì¼ ì²­í¬ ìš”ì•½ í•¨ìˆ˜ (JSON ê°•ì œ) ---
    def _single_chunk(md_text: str) -> dict:
        sys_prompt = (
            "You are a performance review assistant for a PM.\n"
            "Read the entire Notion page (Markdown) and decide:\n"
            "- perf_flag: 'yes' if there is any content usable for performance review; else 'no'.\n"
            "- importance: integer 1~5 (5 is the most impactful).\n"
            "- summary_5lines: exactly 5 bullet-like lines capturing concrete achievements, metrics (%/numbers), decisions, and outcomes.\n"
            f"Write the summary_5lines in language: {summary_lang}.\n"
            "Return ONLY valid JSON with keys: perf_flag, importance, summary_5lines (array of 5 strings)."
        )
        user_prompt = f"# Notion Markdown\n{md_text}\n"

        tries = 0
        last_err = None
        backoff = max(0.1, float(initial_backoff))
        while tries < max_retries:
            tries += 1
            try:
                _throttle(min_interval)
                resp = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": sys_prompt},
                        {"role": "user", "content": user_prompt},
                    ],
                    temperature=0.2,
                    timeout=timeout,
                    response_format={"type": "json_object"},
                )
                text = resp.choices[0].message.content.strip()
                if debug:
                    print("[OpenAI][raw]", text[:500], "..." if len(text) > 500 else "")
                return parse_ai_output(text)
            except KeyboardInterrupt:
                raise
            except Exception as e:
                es = str(e)
                if "insufficient_quota" in es or "You exceeded your current quota" in es:
                    raise e
                last_err = e
                wait = min(max_backoff, backoff) * (1.0 + (0.25 * (time.time() % 1)))
                time.sleep(wait)
                backoff *= 2
        raise last_err

    # --- ê¸¸ë©´ ë§µ-ë¦¬ë“€ìŠ¤ ìš”ì•½ ê²½ë¡œ ---
    text = (markdown_text or "").strip()
    if len(text) > max(2000, int(chunk_threshold)):
        # 1) map: ì²­í¬ë³„ 5ì¤„ ìš”ì•½
        chunks: List[str] = []
        i = 0
        n = len(text)
        size = max(1000, int(chunk_size))
        ov = max(0, int(chunk_overlap))
        while i < n:
            j = min(n, i + size)
            chunks.append(text[i:j])
            if j >= n:
                break
            i = j - ov if (j - ov) > i else j

        inter_lines: List[str] = []
        for idx, ch in enumerate(chunks, start=1):
            try:
                ai = _single_chunk(ch)
                inter_lines.extend([s for s in ai.get("summary_5lines", []) if s])
            except Exception as e:
                append_failure(FAIL_LOG, f"OpenAI chunk ì‹¤íŒ¨: {type(e).__name__}: {e}")

        merged = "\n".join(inter_lines)[:80000] if inter_lines else text[:80000]
        # 2) reduce: ì¤‘ê°„ ìš”ì•½ë“¤ì„ ë‹¤ì‹œ 5ì¤„ë¡œ ì••ì¶•
        reduce_prompt = (
            "You will be given bullet points summarized from different sections of a long document.\n"
            "Synthesize them into exactly 5 lines focused on concrete achievements, metrics, decisions, and outcomes.\n"
            f"Write the 5 lines in language: {summary_lang}.\n"
            "Return ONLY valid JSON with keys: perf_flag, importance (1~5), summary_5lines (array of 5 strings)."
        )
        tries = 0
        last_err = None
        backoff = max(0.1, float(initial_backoff))
        while tries < max_retries:
            tries += 1
            try:
                _throttle(min_interval)
                resp = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": reduce_prompt},
                        {"role": "user", "content": merged},
                    ],
                    temperature=0.2,
                    timeout=timeout,
                    response_format={"type": "json_object"},
                )
                text2 = resp.choices[0].message.content.strip()
                if debug:
                    print("[OpenAI][reduce]", text2[:500], "..." if len(text2) > 500 else "")
                return parse_ai_output(text2)
            except KeyboardInterrupt:
                raise
            except Exception as e:
                es = str(e)
                if "insufficient_quota" in es or "You exceeded your current quota" in es:
                    raise e
                last_err = e
                wait = min(max_backoff, backoff) * (1.0 + (0.25 * (time.time() % 1)))
                time.sleep(wait)
                backoff *= 2
        raise last_err

    # --- ì§§ìœ¼ë©´ ë‹¨ì¼ í˜¸ì¶œ ê²½ë¡œ ---
    return _single_chunk(text)

# -------------------- Hugging Face Summarizer --------------------
_hf_summarizer = None

def get_hf_summarizer(model_name: str = "facebook/bart-large-cnn"):
    """
    Lazy-load a HF summarization pipeline. We import inside to avoid hard dependency
    when the user doesn't pass --use-hf.
    """
    global _hf_summarizer
    if _hf_summarizer is None:
        try:
            from transformers import pipeline
        except Exception as e:
            raise RuntimeError(
                "transformers íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. `pip install transformers torch` í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”."
            ) from e
        _hf_summarizer = pipeline("summarization", model=model_name)
    return _hf_summarizer

def _hf_chunk(text: str, chunk_chars: int = 3500, overlap: int = 200) -> List[str]:
    """
    Rough character-based chunking to avoid exceeding model max length.
    """
    if not text:
        return []
    chunks = []
    i = 0
    n = len(text)
    while i < n:
        j = min(n, i + chunk_chars)
        chunk = text[i:j]
        chunks.append(chunk)
        if j >= n:
            break
        i = j - overlap if j - overlap > i else j
    return chunks

def summarize_with_hf(markdown_text: str,
                      model_name: str = "facebook/bart-large-cnn",
                      max_length: int = 220,
                      min_length: int = 60) -> dict:
    """
    Summarize with Hugging Face pipeline.
    For long content: summarize chunks, then summarize the concatenated chunk-summaries.
    Returns the same schema as OpenAI path: {perf_flag, importance, summary_5lines}.
    """
    text = (markdown_text or "").strip()
    if not text:
        return {"perf_flag": "no", "importance": 1, "summary_5lines": ["", "", "", "", ""]}

    pipe = get_hf_summarizer(model_name=model_name)

    # 1) chunked summarization
    pieces = _hf_chunk(text, chunk_chars=3500, overlap=200)
    if not pieces:
        pieces = [text]

    inter_summaries = []
    for p in pieces:
        try:
            out = pipe(p, max_length=max_length, min_length=min_length, do_sample=False)
            inter_summaries.append(out[0]["summary_text"].strip())
        except Exception as e:
            inter_summaries.append("")

    # 2) merge summaries and summarize again if needed
    merged = "\n".join([s for s in inter_summaries if s]).strip()
    if not merged:
        merged = text[:3500]

    try:
        out2 = pipe(merged, max_length=max_length, min_length=max(30, min_length // 2), do_sample=False)
        final_sum = out2[0]["summary_text"].strip()
    except Exception:
        final_sum = merged

    # 3) performance-centric scoring (reuse local heuristics)
    perf_flag = "yes" if any(k in final_sum.lower() for k in ["ì„±ê³¼","ë§¤ì¶œ","ì „í™˜","ì§€í‘œ","okr","kpi","kr","íš¨ìœ¨","ì„íŒ©íŠ¸"]) else "no"
    if any(k in final_sum.lower() for k in ["í° ì„íŒ©íŠ¸","ë§¤ì¶œ","ì „í™˜ìœ¨","%","ë‹¬ì„±","ëª©í‘œ ë‹¬ì„±","ì§€í‘œ ê°œì„ "]):
        importance = 4 if perf_flag == "yes" else 2
        if any(k in final_sum.lower() for k in ["ì „ì‚¬", "í•µì‹¬", "ëŒ€ê·œëª¨", "í¬ë¦¬í‹°ì»¬", "ì¥ì•  í•´ê²°", "ì¶œì‹œ"]):
            importance = 5
    else:
        importance = 3 if perf_flag == "yes" else 1

    # 4) split into 5 lines
    # prefer sentence-like splits; fallback to newline split
    parts = re.split(r'(?<=[.!?])\s+|\n+', final_sum)
    parts = [p.strip(" \t\n\râ€¢-") for p in parts if p and len(p.strip()) > 2]
    summary_5 = (parts + [""]*5)[:5]

    # length guard per line
    summary_5 = [(s[:240] + "â€¦") if len(s) > 240 else s for s in summary_5]

    return {"perf_flag": perf_flag, "importance": importance, "summary_5lines": summary_5}

# -------------------- CSV I/O --------------------
def ensure_header(path: str):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    if not os.path.exists(path) or os.path.getsize(path) == 0:
        with open(path, "w", newline="", encoding="utf-8") as f:
            w = csv.writer(f)
            w.writerow(["created_date", "title", "id", "url", "perf_flag", "importance", "summary_5lines"])

def read_input(path: str) -> List[dict]:
    rows = []
    with open(path, "r", newline="", encoding="utf-8") as f:
        rdr = csv.DictReader(f)
        for r in rdr:
            rows.append({
                "created_date": (r.get("created_date") or "").strip(),
                "title": (r.get("title") or "").strip(),
                "id": (r.get("id") or "").strip(),
                "url": (r.get("url") or "").strip(),
            })
    return rows

def load_done_ids(path: str) -> Dict[str, dict]:
    done = {}
    if not os.path.exists(path) or os.path.getsize(path) == 0:
        return done
    with open(path, "r", newline="", encoding="utf-8") as f:
        rdr = csv.DictReader(f)
        for r in rdr:
            rid = (r.get("id") or "").strip()
            if not rid:
                continue
            done[rid] = {
                "created_date": (r.get("created_date") or "").strip(),
                "title": (r.get("title") or "").strip(),
                "url": (r.get("url") or "").strip(),
                "perf_flag": (r.get("perf_flag") or "").strip() or "no",
                "importance": (r.get("importance") or "").strip() or "1",
                "summary_5lines": (r.get("summary_5lines") or "").strip(),
            }
    return done

def append_result(path: str, rows: List[dict]):
    if not rows:
        return
    ensure_header(path)
    with open(path, "a", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        for r in rows:
            w.writerow([
                r["created_date"],
                r["title"],
                r["id"],
                r["url"],
                r["perf_flag"],
                r["importance"],
                r["summary_5lines"],
            ])

def backup_existing_output(path: str) -> Optional[str]:
    """If output exists and is non-empty, create a timestamped .bak copy next to it."""
    try:
        if path and os.path.exists(path) and os.path.getsize(path) > 0:
            d = os.path.dirname(path) or "."
            base = os.path.basename(path)
            name, ext = os.path.splitext(base)
            ts = datetime.now().strftime("%Y%m%d-%H%M%S")
            bak = os.path.join(d, f"{name}.bak-{ts}{ext}")
            os.makedirs(d, exist_ok=True)
            shutil.copy2(path, bak)
            log(f"[ë°±ì—…] ê¸°ì¡´ ì¶œë ¥ ë°±ì—… â†’ {bak}")
            return bak
    except Exception as e:
        append_failure(FAIL_LOG, f"ë°±ì—… ì‹¤íŒ¨: {type(e).__name__}: {e}", {"path": path})
    return None

# -------------------- ë©”ì¸ --------------------
def main():
    import argparse

    parser = argparse.ArgumentParser(description="manifest_dedup_sorted.csv â†’ AI enriched CSV")
    parser.add_argument("--input", required=True, help="ì…ë ¥ CSV (manifest_dedup_sorted.csv)")
    parser.add_argument("--output", required=True, help="ì¶œë ¥ CSV (manifest_ai_enriched.csv)")

    # ì¬ì‹¤í–‰/ê±´ë„ˆë›°ê¸° ì •ì±…
    parser.add_argument("--resume", action="store_true", default=False, help="ê¸°ì¡´ ì¶œë ¥ì—ì„œ ì´ì–´ì„œ ì§„í–‰")
    parser.add_argument("--overwrite", action="store_true", default=False, help="ì¶œë ¥ íŒŒì¼ì„ ìƒˆë¡œ ìƒì„±(ê¸°ì¡´ ë¬´ì‹œ)")
    parser.add_argument("--force", "--force-retry-failed", dest="force_retry_failed",
                        action="store_true", default=False,
                        help="ì´ì „ ì‹¤íŒ¨(ê¸°ë³¸ìš”ì•½/ë¹ˆìš”ì•½ í¬í•¨) ê±´ ì¬ì‹œë„")
    parser.add_argument("--only-missing", dest="only_missing",
                        action="store_true", default=False,
                        help="ê¸°ì¡´ ì¶œë ¥ì— ì—†ëŠ” idë§Œ ì²˜ë¦¬")

    # OpenAI ì˜µì…˜
    parser.add_argument("--use-openai", action="store_true", default=False,
                        help="OpenAIë¡œ ìš”ì•½/ìŠ¤ì½”ì–´ ìƒì„±")
    parser.add_argument("--openai-model", type=str, default="gpt-4o-mini",
                        help="OpenAI ëª¨ë¸ëª… (ê¸°ë³¸ gpt-4o-mini)")
    parser.add_argument("--openai-timeout", type=int, default=60,
                        help="OpenAI í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ(ì´ˆ)")
    parser.add_argument("--openai-debug", action="store_true", default=False,
                        help="OpenAI ì‘ë‹µ ì›ë¬¸ ì¼ë¶€ ë¡œê·¸")
    parser.add_argument("--openai-sanity-check", action="store_true", default=False,
                        help="ì‹œì‘ ì‹œ 1íšŒ í…ŒìŠ¤íŠ¸ ì½œ í›„ ì§„í–‰")
    parser.add_argument("--openai-max-retries", type=int, default=8, help="OpenAI ì¬ì‹œë„ ìµœëŒ€ íšŸìˆ˜")
    parser.add_argument("--openai-initial-backoff", type=float, default=1.0, help="OpenAI ì¬ì‹œë„ ì´ˆê¸° ëŒ€ê¸°(ì´ˆ)")
    parser.add_argument("--openai-max-backoff", type=float, default=60.0, help="OpenAI ì¬ì‹œë„ ìµœëŒ€ ëŒ€ê¸°(ì´ˆ)")
    parser.add_argument("--min-ai-interval", type=float, default=0.0, help="OpenAI í˜¸ì¶œ ìµœì†Œ ê°„ê²©(ì´ˆ) - ë ˆì´íŠ¸ë¦¬ë°‹ íšŒí”¼")
    parser.add_argument("--summary-lang", type=str, default="ko", help="ìš”ì•½ ì¶œë ¥ ì–¸ì–´(ko/en ë“±)")
    parser.add_argument("--openai-chunk-threshold", type=int, default=16000, help="ì´ ê¸¸ì´ ì´ìƒì´ë©´ ë§µ-ë¦¬ë“€ìŠ¤ ìš”ì•½")
    parser.add_argument("--openai-chunk-size", type=int, default=6000, help="ì²­í¬ í¬ê¸°(ë¬¸ì ê¸°ì¤€)")
    parser.add_argument("--openai-chunk-overlap", type=int, default=400, help="ì²­í¬ ì˜¤ë²„ë©(ë¬¸ì)")

    # Hugging Face ì˜µì…˜
    parser.add_argument("--use-hf", action="store_true", default=False,
                        help="Hugging Face transformers ìš”ì•½ ì‚¬ìš© (OpenAI ëŒ€ì²´)")
    parser.add_argument("--hf-model", type=str, default="facebook/bart-large-cnn",
                        help="HF summarization ëª¨ë¸ëª… (ex. facebook/bart-large-cnn, google/pegasus-xsum)")
    parser.add_argument("--hf-max-length", type=int, default=220,
                        help="HF ìš”ì•½ max_length")
    parser.add_argument("--hf-min-length", type=int, default=60,
                        help="HF ìš”ì•½ min_length")

    # ì§„í–‰/ì„±ëŠ¥ ì˜µì…˜
    parser.add_argument("--max-blocks", type=int, default=2000, help="ë³¸ë¬¸ ìŠ¤ìº” ìµœëŒ€ ë¸”ë¡ ìˆ˜")
    parser.add_argument("--md-depth", type=int, default=2, help="ë³¸ë¬¸ ìˆ˜ì§‘ ê¹Šì´(1=ì§ê³„ë§Œ, 2 ì´ìƒ ê¶Œì¥)")
    parser.add_argument("--autosave-every", type=int, default=20, help="Nê±´ë§ˆë‹¤ ìë™ ì €ì¥")
    parser.add_argument("--autosave-interval-seconds", type=int, default=30, help="ì´ ì´ˆë§ˆë‹¤ ì£¼ê¸° ì €ì¥(í–‰ ìˆ˜ì™€ ë¬´ê´€)")
    parser.add_argument("--progress-every", type=int, default=50, help="Nê±´ë§ˆë‹¤ ì§„í–‰ ë¡œê·¸")
    parser.add_argument("--per-item-log", action="store_true", default=False, help="ê° í–‰ ì‹œì‘ ë¡œê·¸")

    args = parser.parse_args()

    # Notion í† í° ê²€ì¦(ìš”ì•½ì— í•„ìš”í•œ ë³¸ë¬¸ì„ ë¶ˆëŸ¬ì˜¤ë ¤ë©´ í•„ìš”)
    # .env â†’ Keychain â†’ í”„ë¡¬í”„íŠ¸ ìˆœìœ¼ë¡œ ë³´ì™„ ë¡œë“œ í›„ ê²€ì¦
    _resolve_tokens_from_keychain_or_prompt()
    if not API_TOKEN or API_TOKEN == "PUT_YOUR_INTEGRATION_TOKEN_HERE":
        log("â— NOTION_TOKEN ë¯¸ì„¤ì •: Keychain/í”„ë¡¬í”„íŠ¸ì—ì„œë„ í™•ë³´ ì‹¤íŒ¨")
        sys.exit(2)
    validate_token_ascii(API_TOKEN)

    # OpenAI í‚¤ í™•ì¸ (í•„ìš” ì‹œ Keychain/í”„ë¡¬í”„íŠ¸ ì‹œë„)
    if args.use_openai and not os.environ.get("OPENAI_API_KEY"):
        _resolve_tokens_from_keychain_or_prompt()
        if not os.environ.get("OPENAI_API_KEY"):
            log("â— OPENAI_API_KEY ë¯¸ì„¤ì •: --use-openai ë¥¼ ë„ê±°ë‚˜ Keychain/í”„ë¡¬í”„íŠ¸ë¡œ ì„¤ì •í•˜ì„¸ìš”.")
            sys.exit(2)

    # Ctrl+C ì•ˆì „ ì¢…ë£Œ
    stop_flag = {"stop": False}
    def on_sigint(signum, frame):
        stop_flag["stop"] = True
        log("\nì‚¬ìš©ì ì¤‘ë‹¨: í˜„ì¬ê¹Œì§€ ì§„í–‰ë¶„ ì €ì¥ ì¤‘â€¦")
    signal.signal(signal.SIGINT, on_sigint)

    # ì…ë ¥/ì¶œë ¥ ì¤€ë¹„
    in_rows = read_input(args.input)

    if args.overwrite and os.path.exists(args.output):
        # ì•ˆì „ì„ ìœ„í•´ ë®ì–´ì“°ê¸° ì „ì— ìë™ ë°±ì—… ìƒì„±
        backup_existing_output(args.output)
        os.remove(args.output)
    ensure_header(args.output)

    done_map = {}
    if args.resume and os.path.exists(args.output):
        done_map = load_done_ids(args.output)
        log(f"ì¬ê°œ ëª¨ë“œ: ê¸°ì¡´ ì¶œë ¥ {len(done_map)}ê±´ ë¡œë“œ")

    # OpenAI ìƒŒí‹°í‹° ì²´í¬(ì˜µì…˜)
    if args.use_openai and args.openai_sanity_check:
        try:
            _ = call_openai_summary(
                "This is a sanity check.",
                model=args.openai_model,
                timeout=args.openai_timeout,
                debug=args.openai_debug,
                max_retries=args.openai_max_retries,
                initial_backoff=args.openai_initial_backoff,
                max_backoff=args.openai_max_backoff,
                min_interval=args.min_ai_interval,
            )
            log("[OpenAI] Sanity check OK")
        except KeyboardInterrupt:
            raise
        except Exception as e:
            msg = f"[OpenAI] Sanity check FAILED({type(e).__name__}): {e}"
            log(msg)
            append_failure(FAIL_LOG, msg)
            # 'insufficient_quota' ë“±ì€ ìë™ í´ë°±
            log("â†’ OpenAI ìš”ì•½ì„ ë¹„í™œì„±í™”í•˜ê³  ê¸°ë³¸ ìš”ì•½ìœ¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤. (ë‚˜ì¤‘ì— --force ë¡œ ì¬ì‹œë„ ê°€ëŠ¥)")
            args.use_openai = False
            if getattr(args, "use_hf", False):
                log("â†’ ì´í›„ ëª¨ë“  í•­ëª©ì€ HF ìš”ì•½ê¸°ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            else:
                log("â†’ ì´í›„ ëª¨ë“  í•­ëª©ì€ ë¡œì»¬ ìš”ì•½ê¸°ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        
    total = len(in_rows)
    # ì²˜ë¦¬ ëŒ€ìƒ íŒë³„
    def is_failed_row(v: dict) -> bool:
        s = (v.get("summary_5lines") or "").strip()
        return (not s) or ("AI ìš”ì•½ ì‹¤íŒ¨" in s)

    targets = []
    for r in in_rows:
        rid = r["id"]
        if not rid:
            continue
        if args.only_missing:
            if rid in done_map:
                continue
        elif args.force_retry_failed:
            if rid in done_map and is_failed_row(done_map[rid]):
                pass  # ì¬ì²˜ë¦¬
            elif rid in done_map:
                continue  # ì„±ê³µê±´ì€ ìŠ¤í‚µ
        else:
            if rid in done_map:
                continue
        targets.append(r)

    log(f"ì‹œì‘: ì „ì²´ {total}ê±´ / ì²˜ë¦¬ëŒ€ìƒ {len(targets)}ê±´ "
        f"(resume={'ON' if args.resume else 'OFF'}, overwrite={'ON' if args.overwrite else 'OFF'})")

    processed = 0
    written_since_autosave = 0
    last_autosave_ts = time.time()
    pending_rows: List[dict] = []

    def flush(reason: str):
        nonlocal pending_rows, written_since_autosave
        if pending_rows:
            append_result(args.output, pending_rows)
            pending_rows.clear()
            written_since_autosave = 0
        log(f"[ì €ì¥] {reason} â†’ {args.output}")

    for idx, r in enumerate(targets, start=1):
        if stop_flag["stop"]:
            break

        created_date = r["created_date"]
        title = r["title"]
        nid = r["id"]
        url = r["url"]

        if args.per_item_log:
            log(f"[{idx}/{len(targets)}] start id={nid} title={title}")

        # ë³¸ë¬¸ ìˆ˜ì§‘
        md = ""
        try:
            page = retrieve_page(nid)
            if page:
                md = get_page_markdown(nid, max_blocks=args.max_blocks, depth=args.md_depth)
        except KeyboardInterrupt:
            raise
        except Exception as e:
            append_failure(FAIL_LOG, f"ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {type(e).__name__}: {e}", {"id": nid})

        # ìš”ì•½ ê²½ë¡œ ì„ íƒ: OpenAI â†’ HF â†’ ë¡œì»¬ íœ´ë¦¬ìŠ¤í‹±
        if args.use_openai:
            try:
                ai = call_openai_summary(
                    md[:80000],  # ê³¼ë„í•œ ê¸¸ì´ ë°©ì§€
                    model=args.openai_model,
                    timeout=args.openai_timeout,
                    debug=args.openai_debug,
                    max_retries=args.openai_max_retries,
                    initial_backoff=args.openai_initial_backoff,
                    max_backoff=args.openai_max_backoff,
                    min_interval=args.min_ai_interval,
                    summary_lang=args.summary_lang,
                    chunk_threshold=args.openai_chunk_threshold,
                    chunk_size=args.openai_chunk_size,
                    chunk_overlap=args.openai_chunk_overlap,
                )
                perf_flag = ai["perf_flag"]
                importance = ai["importance"]
                summary_lines = ai["summary_5lines"]
                log(f"[OPENAI] ìš”ì•½ ì„±ê³µ id={nid} imp={importance} perf={perf_flag}")
            except KeyboardInterrupt:
                raise
            except Exception as e:
                print(f"[AI-ERROR] {type(e).__name__}: {e}")
                append_failure(FAIL_LOG, f"OpenAI ì‹¤íŒ¨: {type(e).__name__}: {e}", {"id": nid})
                # OpenAI ì‹¤íŒ¨ ì‹œ HFê°€ ì¼œì ¸ ìˆìœ¼ë©´ HFë¡œ í´ë°±
                if getattr(args, "use_hf", False):
                    try:
                        ai = summarize_with_hf(
                            md[:40000],
                            model_name=args.hf_model,
                            max_length=args.hf_max_length,
                            min_length=args.hf_min_length,
                        )
                        perf_flag = ai["perf_flag"]
                        importance = ai["importance"]
                        summary_lines = ai["summary_5lines"]
                        log(f"[HF-FB] OpenAI ì‹¤íŒ¨ â†’ HF ìš”ì•½ ì‚¬ìš© id={nid} imp={importance} perf={perf_flag}")
                    except Exception as e2:
                        append_failure(FAIL_LOG, f"HF ì‹¤íŒ¨: {type(e2).__name__}: {e2}", {"id": nid})
                        local = summarize_locally(md, title, created_date, url)
                        perf_flag = local["perf_flag"]
                        importance = local["importance"]
                        summary_lines = local["summary_5lines"]
                        log(f"[LOCAL-FB] HF ì‹¤íŒ¨ â†’ ë¡œì»¬ ìš”ì•½ ì‚¬ìš© id={nid} imp={importance} perf={perf_flag}")
                else:
                    local = summarize_locally(md, title, created_date, url)
                    perf_flag = local["perf_flag"]
                    importance = local["importance"]
                    summary_lines = local["summary_5lines"]
                    log(f"[LOCAL-FB] OpenAI ì‹¤íŒ¨ â†’ ë¡œì»¬ ìš”ì•½ ì‚¬ìš© id={nid} imp={importance} perf={perf_flag}")
        elif getattr(args, "use_hf", False):
            try:
                ai = summarize_with_hf(
                    md[:40000],
                    model_name=args.hf_model,
                    max_length=args.hf_max_length,
                    min_length=args.hf_min_length,
                )
                perf_flag = ai["perf_flag"]
                importance = ai["importance"]
                summary_lines = ai["summary_5lines"]
                log(f"[HF] ìš”ì•½ ì‚¬ìš© id={nid} imp={importance} perf={perf_flag}")
            except Exception as e:
                append_failure(FAIL_LOG, f"HF ì‹¤íŒ¨: {type(e).__name__}: {e}", {"id": nid})
                local = summarize_locally(md, title, created_date, url)
                perf_flag = local["perf_flag"]
                importance = local["importance"]
                summary_lines = local["summary_5lines"]
                log(f"[LOCAL-FB] HF ì‹¤íŒ¨ â†’ ë¡œì»¬ ìš”ì•½ ì‚¬ìš© id={nid} imp={importance} perf={perf_flag}")
        else:
            # ë¡œì»¬ íœ´ë¦¬ìŠ¤í‹± ìš”ì•½
            local = summarize_locally(md, title, created_date, url)
            perf_flag = local["perf_flag"]
            importance = local["importance"]
            summary_lines = local["summary_5lines"]
            log(f"[LOCAL] ìš”ì•½ ì‚¬ìš© id={nid} imp={importance} perf={perf_flag}")

        # 1ì ì¸ ê²½ìš° ìš”ì•½ì„ ë‹¨ì¼ ë¼ì¸ìœ¼ë¡œ ì¶•ì•½
        try:
            imp_int = int(importance)
        except Exception:
            imp_int = 1
        if imp_int == 1:
            summary_lines = ["ë³¸ë¬¸ì´ ì§§ê±°ë‚˜ í˜•ì‹ ìœ„ì£¼ë¼ ì„±ê³¼ ìš”ì•½ì´ ì–´ë µìŠµë‹ˆë‹¤."]

        # ê²°ê³¼ ì ì¬
        out_row = {
            "created_date": created_date,
            "title": title,
            "id": nid,
            "url": url,
            "perf_flag": perf_flag,
            "importance": str(importance),
            "summary_5lines": "\n".join(summary_lines),
        }
        pending_rows.append(out_row)
        written_since_autosave += 1
        processed += 1

        # ì§„í–‰ ë¡œê·¸/ìë™ ì €ì¥
        if args.progress_every and (processed % args.progress_every == 0):
            log(f"[ì§„í–‰] {processed}/{len(targets)}")
        # í–‰ ê¸°ì¤€
        if args.autosave_every and (written_since_autosave >= args.autosave_every):
            flush("ìë™ ì €ì¥(í–‰ ê¸°ì¤€)")
            last_autosave_ts = time.time()
        # ì‹œê°„ ê¸°ì¤€
        if args.autosave_interval_seconds and (time.time() - last_autosave_ts >= args.autosave_interval_seconds):
            flush("ìë™ ì €ì¥(ì£¼ê¸° ê¸°ì¤€)")
            last_autosave_ts = time.time()

    # ë§ˆì§€ë§‰ ì €ì¥
    flush("ìµœì¢… ì €ì¥")
    log(f"\nì™„ë£Œ âœ… ê²°ê³¼ ì €ì¥: {args.output}")
    log(f"ì´ ì²˜ë¦¬: {processed}/{len(targets)}")

if __name__ == "__main__":
    try:
        main()
    except SystemExit:
        raise
    except KeyboardInterrupt:
        log("\nì‚¬ìš©ì ì¤‘ë‹¨(KeyboardInterrupt)")
        raise
    except Exception as e:
        append_failure(FAIL_LOG, f"ì¹˜ëª…ì  ì˜¤ë¥˜: {type(e).__name__}: {e}")
        raise
